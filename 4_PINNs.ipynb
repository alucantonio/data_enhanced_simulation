{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alucantonio/data_enhanced_simulation/blob/master/4_PINNs.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics-Informed Neural Networks (PINNs) in JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- Raissi et al.\n",
    "  [paper](https://www.sciencedirect.com/science/article/pii/S0021999118307125)\n",
    "- Comparison between PINNs and standard NNs:\n",
    "  [blog](https://benmoseley.blog/my-research/so-what-is-a-physics-informed-neural-network/)\n",
    "  post by M. Moseley\n",
    "- YouTube [video](https://www.youtube.com/watch?v=-zrY7P2dVC4) by R. Brunton\n",
    "- Medium\n",
    "  [post](https://towardsdatascience.com/solving-differential-equations-with-neural-networks-afdcf7b8bcc4)\n",
    "  by M. Dagrada"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A PINN is a **coordinate network** that attempts to approximate the solution to a\n",
    "partial differential equation $\\hat{u}_\\theta(t, x) \\approx u(t, x)$. The parameters of\n",
    "the network $\\theta^*$ are found by minimizing the following composite loss \n",
    "$$\n",
    "\\theta^* = \\arg\\min_\\theta \\mathcal{L}(\\theta) = \\arg\\min_\\theta \\left( \\mathcal{L}_\\text{PDE}(\\theta) + \\mathcal{L}_\\text{BC}(\\theta) \\right)\n",
    "$$\n",
    "where $\\mathcal{L}_\\text{PDE}(\\theta)$ is the residual of the differential equation\n",
    "computed at **arbitrary points** and\n",
    "$\\mathcal{L}_\\text{BC}(\\theta)$ is the residual of the boundary conditions (_training data_). In transient\n",
    "problems, the residual of the initial conditions is also added to the loss.\n",
    "\n",
    "Typical architectures for PINNs are Multi-Layer Perceptrons, whose inputs are the\n",
    "spatial locations $x$ and time $t$ (if applicable). The output of the network is the\n",
    "solution $\\hat{u}_\\theta(t, x)$. \n",
    "\n",
    "Some key points/limitations:\n",
    "- The physics (differential equation) is not going to be satisfied exactly (loss is a\n",
    "  balance between multiple terms)\n",
    "- The physics-based term makes training harder by altering the loss landscape\n",
    "- PINNs are effective with small amount of training data (i.e. reconstructing flow\n",
    "  fields from sparse measurements)\n",
    "- Fast inference, even though speed of train could be comparable to other methods;\n",
    "  better generalization than standard (non-physics informed) NNs\n",
    "- Derivative information is obtained via higher-order automatic\n",
    "differentiation of the neural network. In particular, to train PINNs to solve PDEs with the highest\n",
    "derivative order being $k$, we need $k+1$ autodiff passes (i.e. applications of the\n",
    "`grad` operator in jax)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: 1D Poisson equation\n",
    "\n",
    "In this simple example we train a PINN in Flax to solve the 1D Poisson equation with homogeneous Dirichlet boundary conditions on the unit interval:\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{d^2 u}{d x^2} &= - f(x), \\quad x \\in (0, 1) \\\\\n",
    "    u(0) &= u(1) = 0\n",
    "\\end{align}\n",
    "$$\n",
    "where the source term is given by\n",
    "$$\n",
    "f(x) = \\begin{cases}\n",
    "    1 & \\text{if } x > 0.3 \\land x < 0.5 \\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The terms of the loss function to minimize to train the PINN are given by\n",
    "$$\n",
    "\\mathcal{L}_\\text{PDE}(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\frac{d^2\n",
    "\\hat{u}_\\theta}{d x^2}\\bigg|_{x=x^{[i]}} + f(x^{[i]}) \\right)^2\n",
    "\\qquad \\text{with} \\qquad\n",
    "x^{[i]} \\sim \\mathcal{U}(0, 1)\n",
    "$$\n",
    "$$\n",
    "\\mathcal{L}_\\text{BC}(\\theta) = \\left( \\hat{u}_\\theta(0) - 0 \\right)^2 + \\left( \\hat{u}_\\theta(1) - 0 \\right)^2.\n",
    "$$\n",
    "where $x^{[i]}$ are the _collocation points_."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A reference solution by Finite Differences\n",
    "\n",
    "To check whether the network is able to learn an accurate solution to the PDE, we\n",
    "compare the solution with that obtained by Finite Differences.\n",
    "\n",
    "1. Discretize the unit interval into $M+2$ _uniformly spaced_ points, such that $\\Delta\n",
    "   x = 1/(M+1)$.\n",
    "2. Solve the linear system of equations $A u = b$ with $A \\in \\mathbb{R}^{M \\times M}$ and $b \\in \\mathbb{R}^N$ with\n",
    "    1. $A$ corresponds to the second order _centered_ approximation for the second derivative (tri-diagonal matrix with $1$ on the off-diagonals and $-2$ on the diagonal, scaled by $\\frac{1}{(\\Delta x)^2}$)\n",
    "    2. $b$ is the right-hand side of the PDE evaluated at the interior points, i.e., $b_i = - f(x_i)$\n",
    "3. Append (pre/post) the boundary conditions to the solution vector $u$ and plot the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import numpy as np\n",
    "from flax import nnx\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm # progress bar\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_interior_nodes = 99\n",
    "dx = 1/(num_interior_nodes+1)\n",
    "BC_LOSS_WEIGHT = 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_full = jnp.linspace(0.0, 1.0, num_interior_nodes + 2)\n",
    "mesh_interior = mesh_full[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhs_function = lambda x: jnp.where((x > 0.3) & (x < 0.5), 1.0, 0.0)\n",
    "rhs_evaluated = rhs_function(mesh_interior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finite Differences solution\n",
    "A = jnp.diag(jnp.ones(num_interior_nodes - 1), -1) + jnp.diag(jnp.ones(num_interior_nodes - 1), 1) - jnp.diag(2 * jnp.ones(num_interior_nodes), 0)\n",
    "A /= dx**2\n",
    "finite_difference_solution = jnp.linalg.solve(A, -rhs_evaluated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding boundary conditions\n",
    "wrap_bc = lambda u: jnp.pad(u, (1, 1), mode=\"constant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mesh_full, wrap_bc(finite_difference_solution), label=\"Finite Difference solution\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PINN solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PINN architecture\n",
    "class Model(nnx.Module):\n",
    "  def __init__(self, rngs: nnx.Rngs):\n",
    "    self.linear1 = nnx.Linear(1, 10, rngs=rngs)\n",
    "    self.linear2 = nnx.Linear(10, 10, rngs=rngs)\n",
    "    self.linear3 = nnx.Linear(10, 10, rngs=rngs)\n",
    "    self.linear4 = nnx.Linear(10, 1, rngs=rngs)\n",
    "\n",
    "  # IMPORTANT: define the forward pass to operate on a single scalar (shape (1,)) and\n",
    "  # return a scalar for jax.grad to work (we will vectorize over samples using vmap)\n",
    "  def __call__(self, x):\n",
    "    y = self.linear1(x)\n",
    "    y = nnx.sigmoid(y) # choose a differentiable activation\n",
    "    y = self.linear2(y)\n",
    "    y = nnx.sigmoid(y)\n",
    "    y = self.linear3(y)\n",
    "    y = nnx.sigmoid(y)\n",
    "    y = self.linear4(y)\n",
    "    return y.squeeze() # shape = (1,) -> shape = ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDE residual computed on a single sample (shape = (1,))\n",
    "def pde_residual(model, x):\n",
    "    grad_x = lambda x: jax.grad(model)(x).squeeze() # shape = (1,) -> shape = (), because grad only works with functions that return scalars\n",
    "    return jax.grad(grad_x)(x) + rhs_function(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physics-informed loss\n",
    "@nnx.jit\n",
    "def loss_fn(model, x):\n",
    "    pde_residual_at_collocation_points = jax.vmap(pde_residual, in_axes=(None, 0))(model, x)\n",
    "    pde_loss_contribution = jnp.mean(jnp.square(pde_residual_at_collocation_points))\n",
    "\n",
    "    left_bc_residual = model(jnp.array([0.0])) - 0.0\n",
    "    right_bc_residual = model(jnp.array([1.0])) - 0.0\n",
    "    bc_residual_contribution = jnp.mean(jnp.square(left_bc_residual)) + jnp.mean(jnp.square(right_bc_residual))\n",
    "\n",
    "    total_loss = pde_loss_contribution + BC_LOSS_WEIGHT * bc_residual_contribution\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def train_step(model, optimizer, x):\n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(model, x)\n",
    "    optimizer.update(grads)\n",
    "    return loss\n",
    "\n",
    "def _train_epoch(\n",
    "    model, optimizer, xs_train, batch_size, rng\n",
    "):\n",
    "    train_ds_size = len(xs_train)\n",
    "    steps_per_epoch = train_ds_size // batch_size\n",
    "\n",
    "    perms = random.permutation(rng, len(xs_train))\n",
    "    perms = perms[: steps_per_epoch * batch_size]\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "    epoch_loss = []\n",
    "\n",
    "    for perm in perms:\n",
    "        batch_xs = xs_train[perm]\n",
    "        loss = train_step(model, optimizer, batch_xs)\n",
    "        epoch_loss.append(loss)  # store training loss for the current batch\n",
    "\n",
    "    train_loss = np.mean(epoch_loss)\n",
    "    return model, train_loss\n",
    "\n",
    "\n",
    "def train(model, optimizer, xs_train, \n",
    "          batch_size,epochs, log_period_epoch=1, show_progress=True):\n",
    "\n",
    "    train_loss_history = []\n",
    "\n",
    "    for epoch in tqdm(range(1, epochs + 1), disable=not show_progress):\n",
    "        model, train_loss = _train_epoch(\n",
    "            model, optimizer, xs_train, \n",
    "            batch_size, random.key(1),\n",
    "        )\n",
    "\n",
    "        train_loss_history.append(train_loss)\n",
    "\n",
    "        if epoch == 1 or epoch % log_period_epoch == 0:\n",
    "            logging.info(\n",
    "                \"epoch:% 3d, train_loss: %.4f\"\n",
    "                % (epoch, train_loss)\n",
    "            )\n",
    "    return train_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.estimator_checks import check_is_fitted\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPRegressor(BaseEstimator):\n",
    "\n",
    "    def __init__(self, model, lr, epochs, batch_size, log_period_epoch=10, show_progress=True):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.log_period_epoch = log_period_epoch\n",
    "        self.show_progress = show_progress\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self._optimizer = nnx.Optimizer(self.model, optax.adam(self.lr))\n",
    "\n",
    "        self.train_loss_history = train(self.model, self._optimizer, X, self.batch_size, self.epochs, self.log_period_epoch, self.show_progress)\n",
    "\n",
    "        self.is_fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self)\n",
    "        return jax.vmap(self.model)(X)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return r2_score(y, y_pred)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "epochs = 10_000\n",
    "num_collocation_points = 50\n",
    "\n",
    "# Create model and wrap into the MLPRegressor (to use sklearn APIs)\n",
    "pinn = Model(rngs=nnx.Rngs(0))\n",
    "m = MLPRegressor(pinn, lr, epochs, num_collocation_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.key(42)\n",
    "collocation_points = random.uniform(key, (num_collocation_points, 1), minval=0.0 + 0.001, maxval=1.0 - 0.001)\n",
    "\n",
    "m.fit(collocation_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(m.train_loss_history)\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mesh_full, wrap_bc(finite_difference_solution), label=\"Finite Difference solution\")\n",
    "plt.plot(mesh_full, pinn(mesh_full.reshape(-1,1)), label=\"Final PINN solution\")\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.score(mesh_full.reshape(-1,1), wrap_bc(finite_difference_solution))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Burgers' equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the following viscous Burgers equation:\n",
    "\\begin{align}\n",
    "  &u_t + uu_x -\\nu u_{xx} = 0 \\quad \\text{for } x\\in [-1,1], t\\in[0, 1] \\\\\n",
    "  &u(x,0) = -\\sin(\\pi x) \\quad \\text{for } x\\in [-1,1]\\\\\n",
    "  &u(-1,t) = u(1,t) = 0 \\quad \\text{for } t\\in [0,1]\n",
    "\\end{align}\n",
    "where $\\nu$ is the PDE parameter. In this section, we take $\\nu=0.025$.\n",
    "\n",
    "The loss function, which includes the loss the IC/BC and the PDE residuals, now reads:\n",
    "\\begin{align}\n",
    "  \\mathcal{L} &= \\mathcal{L}_{\\text{PDE}} + \\mathcal{L}_{\\text{IC}} + \\mathcal{L}_{\\text{BC}}\\\\\n",
    "    &= \\dfrac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}[\\hat{u}_{t}(x^i, t^i) + \\hat{u}\\hat{u}_{x}(x^i, t^i)-\\nu \\hat{u}_{xx}(x^i, t^i)]^2 + \\dfrac{1}{N_{ic}}\\sum_{i=1}^{N_{ic}}(\\hat{u}(x^i,0) + \\sin(\\pi x^i))^2 + \\dfrac{1}{N_{bc}}\\sum_{i=1}^{N_{bc}}(\\hat{u}(-1,t^i)^2+\\hat{u}(1,t^i)^2).\n",
    "\\end{align}\n",
    "\n",
    "We take the following steps to build a PINN model:\n",
    "1. Define the domain\n",
    "2. Define the initial/boundary conditions (IC/BC) and the training points\n",
    "3. Define PDE residuals and loss term for the PDE\n",
    "4. Define PINNs architecture\n",
    "5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretization of the domain (mesh)\n",
    "nx = 256\n",
    "nt = 100\n",
    "x = np.linspace(-1, 1, nx)\n",
    "t = np.linspace(0, 1, nt)\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
    "\n",
    "# Viscosity\n",
    "nu = 0.025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define the IC/BC and training points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For boundary and initial conditions define the matrices of the (x,t) points\n",
    "# corresponding to the boundaries where BCs or initial conditions are prescribe and the\n",
    "# corresponding values (u_bc and u_init) of the solution.\n",
    "# X_init = ...\n",
    "# u_init = ...\n",
    "# X_bc = ...\n",
    "# u_bc = ...\n",
    "\n",
    "num_collocation_points = 2000\n",
    "# Collocation points: matrix of randomly chosen (x,t) pairs from the X_star matrix \n",
    "# X_colloc_train = ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution:\n",
    "\n",
    "# Define the IC\n",
    "idx_init = np.where(X_star[:, 1]==0)[0]\n",
    "X_init = X_star[idx_init]\n",
    "u_init = -np.sin(np.pi*X_init[:, 0:1])\n",
    "\n",
    "# Define the BC\n",
    "idx_bc = np.where((X_star[:, 0]==1.0)|(X_star[:, 0]==-1.0))[0]\n",
    "X_bc = X_star[idx_bc]\n",
    "u_bc = np.zeros((X_bc.shape[0], 1))\n",
    "\n",
    "# Define training collocation points\n",
    "idx_Xf = np.random.choice(X_star.shape[0], num_collocation_points, replace=False)\n",
    "X_colloc_train = X_star[idx_Xf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of the training (supervised) and the collocation (non-supervised) points\n",
    "X_supervised = np.concatenate((X_init, X_bc), axis=0)\n",
    "u_supervised = np.concatenate((u_init, u_bc), axis=0)\n",
    "\n",
    "fig = plt.figure(figsize=[7,3])\n",
    "axes = fig.subplots(1,2, sharex=False, sharey=False)\n",
    "img1 = axes[0].scatter(X_supervised[:, 0:1], X_supervised[:, 1:2], c=u_supervised, cmap='jet', vmax=1, vmin=-1, s=5)\n",
    "axes[0].set_title('Supervised points', fontsize=10)\n",
    "axes[0].set_xlabel('x', fontsize=10)\n",
    "axes[0].set_ylabel('t', fontsize=10)\n",
    "img2 = axes[1].plot(X_colloc_train[:, 0], X_colloc_train[:, 1], 'x', markersize=5, color='black')\n",
    "axes[1].set_title('Collocation/Non-supervised points', fontsize=10)\n",
    "axes[1].set_xlabel('x', fontsize=10)\n",
    "axes[1].set_ylabel('t', fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nnx.Module):\n",
    "  def __init__(self, rngs: nnx.Rngs):\n",
    "    # Define linear layers here (notice the number of input/output features)\n",
    "    # self.linear1 = nnx.Linear(2, ..., rngs=rngs)\n",
    "    # self.linear2 = nnx.Linear(..., 1, rngs=rngs)\n",
    "    pass\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # Define the forward pass for a single sample, i.e. (x,t) pair stored in an array \n",
    "    # with shape (2,)\n",
    "    # return y.squeeze()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution:\n",
    "\n",
    "class Model(nnx.Module):\n",
    "  def __init__(self, rngs: nnx.Rngs):\n",
    "    self.linear1 = nnx.Linear(2, 20, rngs=rngs)\n",
    "    self.linear2 = nnx.Linear(20, 20, rngs=rngs)\n",
    "    self.linear3 = nnx.Linear(20, 20, rngs=rngs)\n",
    "    self.linear4 = nnx.Linear(20, 1, rngs=rngs)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    y = self.linear1(x)\n",
    "    y = nnx.tanh(y)\n",
    "    y = self.linear2(y)\n",
    "    y = nnx.tanh(y)\n",
    "    y = self.linear3(y)\n",
    "    y = nnx.tanh(y)\n",
    "    y = self.linear4(y)\n",
    "    return y.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement the calculation of the PDE residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pde_residual(model, x):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution:\n",
    "\n",
    "def pde_residual(model, x):\n",
    "    grad_x = lambda x: jax.grad(model)(x)[0]\n",
    "    grad_t = lambda x: jax.grad(model)(x)[1]\n",
    "    u_x = grad_x(x)\n",
    "    u_t = grad_t(x)\n",
    "    u_xx = jax.grad(grad_x)(x)[0]\n",
    "    u = model(x)\n",
    "    f = u_t + u*u_x - nu*u_xx\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def loss_fn(model, x):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution:\n",
    "\n",
    "@nnx.jit\n",
    "def loss_fn(model, x):\n",
    "    pde_residual_at_collocation_points = jax.vmap(pde_residual, in_axes=(None, 0))(model, x)\n",
    "    pde_loss_contribution = jnp.mean(jnp.square(pde_residual_at_collocation_points))\n",
    "\n",
    "    bc_residual = model(X_bc)\n",
    "    bc_residual_contribution = jnp.mean(jnp.square(bc_residual))\n",
    "\n",
    "    init_residual = model(X_init) + jnp.sin(jnp.pi*X_init[:,0])\n",
    "    init_residual_contribution = jnp.mean(jnp.square(init_residual))\n",
    "\n",
    "    total_loss = pde_loss_contribution + bc_residual_contribution + init_residual_contribution\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create an instance of the PINN and wrap into the `MLPRegressor` class. Train the\n",
    "  model. NOTE: if you experience issues, you may need to restart the kernel and/or re-evaluate the cells above where the training functions are defined,\n",
    "  befor running the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...\n",
    "# m = MLPRegressor(...)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution:\n",
    "\n",
    "pinn = Model(nnx.Rngs(0))\n",
    "m = MLPRegressor(pinn, 0.001, 5000, 64)\n",
    "m.fit(X_colloc_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot the training curve (history of the training loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(m.train_loss_history)\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compare the prediction of the trained PINN over all the mesh `X_star` with the\n",
    "  reference solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reference solution\n",
    "u_star_array = np.load('./data/burgers_sol.npy', allow_pickle=True)\n",
    "u_star = u_star_array[-1]\n",
    "\n",
    "# Compute PINN prediction\n",
    "# pred = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution:\n",
    "\n",
    "pred = m.predict(X_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and compare the solutions\n",
    "fig = plt.figure(figsize=[15,3])\n",
    "axes = fig.subplots(1,3, sharex=False, sharey=False)\n",
    "img1 = axes[0].scatter(X_star[:, 0:1], X_star[:, 1:2], c=u_star, cmap='jet', vmax=1, vmin=-1, s=5)\n",
    "axes[0].set_title('Reference solution', fontsize=15)\n",
    "axes[0].set_xlabel('x', fontsize=15)\n",
    "axes[0].set_ylabel('t', fontsize=15)\n",
    "plt.colorbar(img1, ax=axes[0])\n",
    "img2 = axes[1].scatter(X_star[:, 0:1], X_star[:, 1:2], c=pred, cmap='jet', vmax=1, vmin=-1, s=5)\n",
    "axes[1].set_title('PINN prediction', fontsize=15)\n",
    "axes[1].set_xlabel('x', fontsize=15)\n",
    "axes[1].set_ylabel('t', fontsize=15)\n",
    "plt.colorbar(img2, ax=axes[1])\n",
    "img3 = axes[2].scatter(X_star[:, 0:1], X_star[:, 1:2], c=u_star-pred[:,None], cmap='seismic', vmax=0.01, vmin=-0.01, s=5)\n",
    "axes[2].set_title('Error', fontsize=15)\n",
    "axes[2].set_xlabel('x', fontsize=15)\n",
    "axes[2].set_ylabel('t', fontsize=15)\n",
    "plt.colorbar(img3, ax=axes[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate the R^2 score between the predictions and the reference solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution:\n",
    "\n",
    "m.score(X_star, u_star.ravel())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "265371ff1b98b9f4eaa16d44fb1eb5bb5e02f4557e1c68186d1d500959ccd159"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

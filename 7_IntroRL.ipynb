{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alucantonio/data_enhanced_simulation/blob/master/7_IntroRL.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- Zai & Brown's \"Deep Reinforcement Learning in Action\"\n",
    "- Thomas Simonini's [course](https://huggingface.co/learn/deep-rl-course/en/unit0/introduction)\n",
    "- David Silver's RL [course](https://www.davidsilver.uk/teaching/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning (RL) is a branch of machine learning. The idea behind RL is that\n",
    "an **agent** (an AI) will learn from the **environment** by interacting with it (through\n",
    "trial and error) and receiving **rewards** (negative or positive) as feedback for\n",
    "performing actions. RL is a framework for solving control tasks (also called **decision\n",
    "problems**). \n",
    "\n",
    "Let's look at the definition of the key terms.\n",
    "\n",
    "A **reward** $R_t$ is a scalar feedback signal that indicates how well agent is doing at step $t$\n",
    "- The agent’s job is to maximise cumulative reward\n",
    "- Actions may have long term consequences\n",
    "- It may be better to sacrifice immediate reward to gain more long-term reward\n",
    "\n",
    "RL is based on the _reward hypothesis_: All goals can be described by the maximisation of expected\n",
    "cumulative reward.\n",
    "\n",
    "A **state** is the set of information available in the environment that can be\n",
    "used to make decisions.\n",
    "- **Markov decision process (MDP)**: the current state alone contains enough information to\n",
    "choose optimal actions to maximize future rewards (we can forget the history of state-actions).\n",
    "\n",
    "A **policy** characterizes the agent's behavior in some environment. It is a map between\n",
    "states and actions (can be a probability distribution). The probability of each action in the distribution is\n",
    "the probability that the action will produce the greatest reward. \n",
    "The _optimal policy_ is the strategy that maximizes the rewards.\n",
    "\n",
    "The **value** function gives a prediction of future reward. It maps a state or a\n",
    "state-action pair into the expected reward (long-term average of\n",
    "rewards received after being in some state or taking some action). When we speak of\n",
    "_the_ value function, we usually mean a state-value function. The value function depends\n",
    "on the policy used to take actions. \n",
    "\n",
    "Examples of problems solvable via RL:\n",
    "- Fly stunt manoeuvres in a helicopter\n",
    "- Defeat the world champion at Backgammon\n",
    "- Manage an investment portfolio\n",
    "- Control a power station\n",
    "- Make a humanoid robot walk\n",
    "- Play many diﬀerent Atari games better than humans\n",
    "\n",
    "\n",
    "RL algorithm can, in principle, employ any statistical learning model, but it has become\n",
    "increasingly popular and effective to use deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the multi-arm bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider $n$ slot machines, each having 1 lever. We call this situation an $n$-armed\n",
    "bandit. We have $n$ possible actions (here $n = 10$) where an action means pulling the\n",
    "arm, or lever, of a particular slot machine, and at each play ($k$) of this game we can\n",
    "choose a single lever to pull. After taking an action ($a$) we will receive a reward,\n",
    "$R_k$ (reward at play $k$). Each lever has a unique probability distribution of \n",
    "payouts (rewards).\n",
    "\n",
    "We implement our reward probability distributions for each arm in the following way: \n",
    "Each arm will have a probability, e.g., 0.7, and the maximum reward is $10.\n",
    "We will set up a for loop going to 10, and at each step it will add 1 to the reward if a\n",
    "random float is less than the arm’s probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(prob, n=10):\n",
    "    reward = 0;\n",
    "    for i in range(n):\n",
    "        if random.random() < prob:\n",
    "            reward += 1\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward distribution for a 10-armed bandit with probability 0.7\n",
    "reward_test = [get_reward(0.7) for _ in range(2000)]\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.xlabel(\"Reward\")\n",
    "plt.ylabel(\"# Observations\")\n",
    "plt.hist(reward_test,bins=9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our strategy should be to play a few times, choosing different levers and observing\n",
    "our rewards for each action. Then we want to only choose the lever with the largest\n",
    "observed average reward.\n",
    "\n",
    "- **Action-value function** $Q_k(a)$: gives the value of the action $a$ at play $k$ as the\n",
    "arithmetic mean of all previous rewards received for taking action $a$ (**expected\n",
    "reward**).\n",
    "\n",
    "This method of simply choosing the best lever that we know of so far\n",
    "is called a **greedy** (or exploitation) method. We need some **exploration** \n",
    "of other levers (other slot machines) to discover the true best action.\n",
    "\n",
    "We can use an **(epsilon)-greedy** strategy, such that with a probability, epsilon, \n",
    "we will choose an action, a, and the rest of the time we will choose the best lever\n",
    "(maximizes action-value function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_record(record,action,r):\n",
    "    # compute new average reward\n",
    "    new_r = (record[action,0] * record[action,1] + r) / (record[action,0] + 1)\n",
    "    record[action,0] += 1\n",
    "    record[action,1] = new_r\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_arm(record):\n",
    "    arm_index = np.argmax(record[:,1],axis=0)\n",
    "    return arm_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1)\n",
    "ax.set_xlabel(\"Plays\")\n",
    "ax.set_ylabel(\"Avg Reward\")\n",
    "fig.set_size_inches(9,5)\n",
    "\n",
    "n = 10\n",
    "eps = 0.2 # epsilon parameter\n",
    "probs = np.random.rand(n) # bandit probabilities\n",
    "\n",
    "# for each arm store the number of times pulled (column 0) and the average reward\n",
    "# (column 1)\n",
    "record = np.zeros((n,2)) \n",
    "\n",
    "rewards = [0]\n",
    "# play games\n",
    "for i in range(500):\n",
    "    # epsilon-greedy strategy\n",
    "    if random.random() > eps:\n",
    "        choice = get_best_arm(record)\n",
    "    else:\n",
    "        choice = np.random.randint(n)\n",
    "\n",
    "    r = get_reward(probs[choice])\n",
    "    record = update_record(record,choice,r)\n",
    "    mean_reward = ((i+1) * rewards[-1] + r)/(i+2)\n",
    "    rewards.append(mean_reward)\n",
    "\n",
    "ax.scatter(np.arange(len(rewards)),rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the $\\epsilon$-greedy strategy, we can use a _softmax_ selection policy to avoid choosing the worst action, but still keeping some ability\n",
    "to explore to find the best one (we don't need `get_best_arm`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(av, tau=1.12):\n",
    "    # takes the action-value vector (av) as input; tau controls the \"temperature\"\n",
    "    softm = (np.exp(av / tau) / np.sum(np.exp(av / tau)))\n",
    "    return softm\n",
    "\n",
    "probs = np.random.rand(n)\n",
    "record = np.zeros((n,2))\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.set_xlabel(\"Plays\")\n",
    "ax.set_ylabel(\"Avg Reward\")\n",
    "fig.set_size_inches(9,5)\n",
    "rewards = [0]\n",
    "for i in range(500):\n",
    "    p = softmax(record[:,1],tau=0.7)\n",
    "    choice = np.random.choice(np.arange(n),p=p)\n",
    "    r = get_reward(probs[choice])\n",
    "    record = update_record(record,choice,r)\n",
    "    mean_reward = ((i+1) * rewards[-1] + r)/(i+2)\n",
    "    rewards.append(mean_reward)\n",
    "\n",
    "ax.scatter(np.arange(len(rewards)),rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: introducing state (contextual bandits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the _contextual bandit_ problem, there is a different reward distribution over the\n",
    "actions for each state.  For simplicity, the number of states equals\n",
    "the number of arms, but in general the state space is often much larger than the\n",
    "action space. Here, we have $n$ different reward distributions\n",
    "over actions for each of $n$ states. \n",
    "\n",
    "Instead of storing the rewards for each state-action pair, we use a neural\n",
    "network to learn the relation between state-action and reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random, vmap\n",
    "import numpy as np\n",
    "from flax import nnx\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary functions\n",
    "def softmax(av, tau=1.12):\n",
    "    # takes the action-value vector (av) as input; tau controls the \"temperature\"\n",
    "    softm = (jnp.exp(av / tau) / jnp.sum(jnp.exp(av / tau)))\n",
    "    return softm\n",
    "\n",
    "def one_hot(N, pos, val=1):\n",
    "    one_hot_vec = jnp.zeros(N)\n",
    "    one_hot_vec = one_hot_vec.at[pos].set(val)\n",
    "    return one_hot_vec\n",
    "\n",
    "def running_mean(x,N=50):\n",
    "    c = x.shape[0] - N\n",
    "    y = np.zeros(c)\n",
    "    conv = np.ones(N)\n",
    "    for i in range(c):\n",
    "        y[i] = (x[i:i+N] @ conv)/N\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Complete the implementation of the `ContextBandit` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextBandit:\n",
    "    def __init__(self, seed=42, arms=10):\n",
    "        key = random.key(seed)\n",
    "        self.arms = arms\n",
    "        self.init_distribution(key, arms)\n",
    "        \n",
    "    def init_distribution(self, key, arms):\n",
    "        # Num states = Num Arms to keep things simple\n",
    "        # each row represents a state, each column an arm\n",
    "        self.bandit_matrix = random.uniform(key, shape=(arms,arms))\n",
    "\n",
    "    def reward(self, prob, key):\n",
    "        subkeys = random.split(key, self.arms + 1)\n",
    "        key = subkeys[0]\n",
    "\n",
    "        # Compute the reward by \"collecting\" payouts from all arms\n",
    "        reward = 0.\n",
    "\n",
    "        return reward, key \n",
    "\n",
    "    def update_state(self, key):\n",
    "        key, subkey = random.split(key)\n",
    "        # return state randomly sampled from uniform distribution\n",
    "        # state = ... \n",
    "        return state, key\n",
    "        \n",
    "    def get_reward(self,arm,state,key):\n",
    "        # Get the reward associated from choosing a certain arm from the given state\n",
    "        # (based on the bandit_matrix distribution)\n",
    "        # ...\n",
    "        pass\n",
    "        \n",
    "    def choose_arm(self, arm, state, key):\n",
    "        # Get the reward and update the state\n",
    "        # ...\n",
    "        # ...\n",
    "        return reward, state, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution:\n",
    "\n",
    "class ContextBandit:\n",
    "    def __init__(self, seed=42, arms=10):\n",
    "        key = random.key(seed)\n",
    "        self.arms = arms\n",
    "        self.init_distribution(key, arms)\n",
    "        \n",
    "    def init_distribution(self, key, arms):\n",
    "        # Num states = Num Arms to keep things simple\n",
    "        # each row represents a state, each column an arm\n",
    "        self.bandit_matrix = random.uniform(key, shape=(arms,arms))\n",
    "\n",
    "    def reward(self, prob, key):\n",
    "        subkeys = random.split(key, self.arms + 1)\n",
    "        key = subkeys[0]  # Update key for future calls\n",
    "\n",
    "        # Use vmap to apply random.uniform with each subkey\n",
    "        rewards = vmap(lambda k: random.uniform(k) < prob)(subkeys[1:])\n",
    "        \n",
    "        return rewards.sum(), key \n",
    "\n",
    "    def update_state(self, key):\n",
    "        # return state randomly sampled from uniform distribution\n",
    "        key, subkey = random.split(key)\n",
    "        state = random.randint(subkey, shape=(), minval=0, maxval=self.arms)\n",
    "        return state, key\n",
    "        \n",
    "    def get_reward(self,arm,state,key):\n",
    "        return self.reward(self.bandit_matrix[state,arm], key)\n",
    "        \n",
    "    def choose_arm(self, arm, state, key):\n",
    "        reward, key = self.get_reward(arm, state, key)\n",
    "        state, key = self.update_state(key)\n",
    "        return reward, state, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "arms = 10\n",
    "N, D_in, H, D_out = 1, arms, 100, arms\n",
    "env = ContextBandit(arms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a NN model that takes as input a one-hot encoded vector of the state and\n",
    "   returns the values (rewards) associated to choosing each arm from that state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nnx.Module):\n",
    "  def __init__(self, D_in, D_out, rngs: nnx.Rngs):\n",
    "    pass\n",
    "\n",
    "  def __call__(self, x):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution:\n",
    "\n",
    "class Model(nnx.Module):\n",
    "  def __init__(self, D_in, D_out, rngs: nnx.Rngs):\n",
    "    self.linear1 = nnx.Linear(D_in, H, rngs=rngs)\n",
    "    self.linear2 = nnx.Linear(H, D_out, rngs=rngs)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    y = nnx.relu(self.linear1(x))\n",
    "    y = nnx.relu(self.linear2(y))\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define the training loop for the NN model, where at each epoch an action is chosen _probabilistically_\n",
    "   based on the predicted rewards from the model and the model weights are updated based\n",
    "   on the _actual_ reward obtained by taking that action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution:\n",
    "\n",
    "@nnx.jit\n",
    "def _train_epoch(model, optimizer, cur_state, key):\n",
    "    # one-hot encoding of the current state\n",
    "    oh_cur_state = one_hot(arms, cur_state)\n",
    "\n",
    "    def loss(model, cur_state, key):\n",
    "        # predict rewards for each action\n",
    "        y_pred = model(oh_cur_state)\n",
    "\n",
    "        # convert rewards to probability distribution\n",
    "        av_softmax = softmax(y_pred, tau=2.0)\n",
    "\n",
    "        # choose new action probabilistically\n",
    "        key, subkey = random.split(key)\n",
    "        choice = random.choice(key=subkey, a=arms, p=av_softmax)\n",
    "        cur_reward, cur_state, key = env.choose_arm(choice, cur_state, key)\n",
    "\n",
    "        # update one_hot_reward to use it as labeled training data\n",
    "        reward = y_pred.at[choice].set(cur_reward)\n",
    "        return optax.losses.squared_error(y_pred, reward).mean(), (cur_reward, cur_state, key)\n",
    "\n",
    "    # update network weights based on MSE between predicted and actual rewards \n",
    "    grads, res = nnx.grad(loss, has_aux=True)(model, cur_state, key)\n",
    "    optimizer.update(grads)\n",
    "\n",
    "    return res\n",
    "\n",
    "def train(env, model, optimizer, epochs=5000):\n",
    "    # initialize random state\n",
    "    key = random.key(6)\n",
    "    cur_state, key = env.update_state(key)\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    for _ in range(epochs):\n",
    "\n",
    "        cur_reward, cur_state, key = _train_epoch(model, optimizer, cur_state, key)\n",
    "\n",
    "        rewards.append(cur_reward)\n",
    "\n",
    "    return np.array(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Train the model for 5000 epochs and plot the running mean (use the auxiliary function\n",
    "   defined above) of the rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution:\n",
    "\n",
    "model = Model(D_in, D_out, rngs=nnx.Rngs(0))\n",
    "\n",
    "lr = 0.01 # learning rate\n",
    "optimizer = nnx.Optimizer(model, optax.adam(lr))\n",
    "\n",
    "rewards = train(env, model, optimizer)\n",
    "plt.plot(running_mean(rewards,N=500))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

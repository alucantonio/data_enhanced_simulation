{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCartPole-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Train the dynamics model beforehand\u001b[39;00m\n\u001b[1;32m     55\u001b[0m dynamics_model \u001b[38;5;241m=\u001b[39m DynamicsModel(state_dim\u001b[38;5;241m=\u001b[39menv\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \n\u001b[0;32m---> 56\u001b[0m                                 action_dim\u001b[38;5;241m=\u001b[39m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Pre-train dynamics model with data (replace `data` with actual training data)\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# dynamics_model.model.fit(inputs, targets)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Wrap the real environment\u001b[39;00m\n\u001b[1;32m     61\u001b[0m model_based_env \u001b[38;5;241m=\u001b[39m ModelBasedEnvWrapper(env, dynamics_model, reward_function)\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "class ModelBasedEnvWrapper(gym.Env):\n",
    "    \"\"\"\n",
    "    A custom environment that uses a pre-trained dynamics model for state transitions.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, dynamics_model, reward_function):\n",
    "        super(ModelBasedEnvWrapper, self).__init__()\n",
    "        self.env = env\n",
    "        self.dynamics_model = dynamics_model  # Pre-trained model for state transitions\n",
    "        self.reward_function = reward_function  # Direct reward access from the real environment\n",
    "        self.observation_space = env.observation_space\n",
    "        self.action_space = env.action_space\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = self.env.reset()\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Use the dynamics model for state transition\n",
    "        next_state = self.dynamics_model.predict(self.current_state, action)\n",
    "        reward = self.reward_function(self.current_state, action)  # Use real reward\n",
    "        self.current_state = next_state\n",
    "        done = False  # Dynamics-based rollouts typically do not handle termination conditions\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "# Define your pre-trained dynamics model\n",
    "class DynamicsModel:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        # Example: Simple neural network-based dynamics model\n",
    "        self.model = self.build_model(state_dim, action_dim)\n",
    "    \n",
    "    def build_model(self, state_dim, action_dim):\n",
    "        import tensorflow as tf\n",
    "        inputs = tf.keras.layers.Input(shape=(state_dim + action_dim,))\n",
    "        x = tf.keras.layers.Dense(64, activation='relu')(inputs)\n",
    "        x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "        outputs = tf.keras.layers.Dense(state_dim)(x)\n",
    "        return tf.keras.models.Model(inputs, outputs)\n",
    "    \n",
    "    def predict(self, state, action):\n",
    "        inputs = np.hstack([state, action])\n",
    "        return self.model.predict(inputs.reshape(1, -1)).flatten()\n",
    "\n",
    "# Reward function (real reward from environment)\n",
    "def reward_function(state, action):\n",
    "    # Example reward function\n",
    "    goal = np.array([0.0, 0.0])\n",
    "    return -np.linalg.norm(state - goal)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "# Train the dynamics model beforehand\n",
    "dynamics_model = DynamicsModel(state_dim=env.observation_space.shape[0], \n",
    "                                action_dim=env.action_space.shape[0])\n",
    "# Pre-train dynamics model with data (replace `data` with actual training data)\n",
    "# dynamics_model.model.fit(inputs, targets)\n",
    "\n",
    "# Wrap the real environment\n",
    "model_based_env = ModelBasedEnvWrapper(env, dynamics_model, reward_function)\n",
    "\n",
    "# Train policy using Stable-Baselines3 PPO\n",
    "model = PPO(\"MlpPolicy\", model_based_env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Use the trained policy\n",
    "obs = model_based_env.reset()\n",
    "for _ in range(1000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, info = model_based_env.step(action)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

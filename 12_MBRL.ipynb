{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC-Dyna-Q Algorithm\n",
    "\n",
    "## Initialization\n",
    "1. Initialize environment $ \\text{env} $.\n",
    "2. Initialize SAC agent $ \\pi_\\theta, Q_\\phi^1, Q_\\phi^2 $ and replay buffer $ \\mathcal{D} $.\n",
    "3. Set $ N_\\text{real\\_episodes}, N_\\text{synthetic\\_samples} $.\n",
    "\n",
    "## Training Loop\n",
    "For $ \\text{episode} $ in $ N_\\text{real\\_episodes} $:\n",
    "1. **Collect Real Data:**\n",
    "   - Reset environment $ s_0 \\leftarrow \\text{env.reset()} $.\n",
    "   - For each step in the environment:\n",
    "     1. Select $ a_t \\sim \\pi_\\theta(a|s_t) $.\n",
    "     2. Execute $ a_t $, observe $ s_{t+1}, r_t, \\text{done} $.\n",
    "     3. Store $ (s_t, a_t, r_t, s_{t+1}, \\text{done}) $ in $ \\mathcal{D} $.\n",
    "\n",
    "2. **Model-Free SAC Update:**\n",
    "   - Sample $ (s, a, r, s', \\text{done}) $ from $ \\mathcal{D} $.\n",
    "   - Update $ Q_\\phi^1, Q_\\phi^2, \\pi_\\theta $ using SAC objectives.\n",
    "\n",
    "3. **Generate Synthetic Data:**\n",
    "   - For $ i $ in $ N_\\text{synthetic\\_samples} $:\n",
    "     1. Sample $ s $ from $ \\mathcal{D} $ or the observation space.\n",
    "     2. Predict $ a \\sim \\pi_\\theta(a|s) $.\n",
    "     3. Simulate $ s', r $ using the known dynamics:\n",
    "        $$\n",
    "        v_{t+1} = v_t + 0.0015 a_t - 0.0025 \\cos(3 x_t), \\quad x_{t+1} = x_t + v_t\n",
    "        $$\n",
    "     4. Store $ (s, a, r, s', \\text{done}) $ in $ \\mathcal{D} $.\n",
    "\n",
    "4. **SAC Update with Synthetic Data:**\n",
    "   - Repeat step 2 with synthetic data added to $ \\mathcal{D} $.\n",
    "\n",
    "## Testing\n",
    "- Evaluate $ \\pi_\\theta $ by running multiple episodes in the real environment.\n",
    "- Compute the average reward over all episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.envs.classic_control import utils\n",
    "\n",
    "class ModelEnv(gym.Env):\n",
    "\n",
    "    def __init__(self, render_mode: Optional[str] = None, g=10.0):\n",
    "        self.max_speed = 8\n",
    "        self.max_torque = 2.0\n",
    "        self.dt = 0.05\n",
    "        self.g = g\n",
    "        self.m = 1.0\n",
    "        self.l = 1.0\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.screen_dim = 500\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        self.isopen = True\n",
    "\n",
    "        high = np.array([1.0, 1.0, self.max_speed], dtype=np.float32)\n",
    "        # This will throw a warning in tests/envs/test_envs in utils/env_checker.py as the space is not symmetric\n",
    "        #   or normalised as max_torque == 2 by default. Ignoring the issue here as the default settings are too old\n",
    "        #   to update to follow the gymnasium api\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-self.max_torque, high=self.max_torque, shape=(1,), dtype=np.float32\n",
    "        )\n",
    "        self.observation_space = spaces.Box(low=-high, high=high, dtype=np.float32)\n",
    "\n",
    "    def step(self, u):\n",
    "        th, thdot = self.state  # th := theta\n",
    "\n",
    "        g = self.g\n",
    "        m = self.m\n",
    "        l = self.l\n",
    "        dt = self.dt\n",
    "\n",
    "        u = np.clip(u, -self.max_torque, self.max_torque)[0]\n",
    "        self.last_u = u  # for rendering\n",
    "        costs = angle_normalize(th) ** 2 + 0.1 * thdot**2 + 0.001 * (u**2)\n",
    "\n",
    "        newthdot = thdot + (3 * g / (2 * l) * np.sin(th) + 3.0 / (m * l**2) * u) * dt\n",
    "        newthdot = np.clip(newthdot, -self.max_speed, self.max_speed)\n",
    "        newth = th + newthdot * dt\n",
    "\n",
    "        self.state = np.array([newth, newthdot])\n",
    "\n",
    "        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\n",
    "        return self._get_obs(), -costs, False, False, {}\n",
    "\n",
    "    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "        super().reset(seed=seed)\n",
    "        if options is None:\n",
    "            high = np.array([np.pi, 1.])\n",
    "        low = -high  # We enforce symmetric limits.\n",
    "        self.state = self.np_random.uniform(low=low, high=high)\n",
    "        self.last_u = None\n",
    "\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        theta, thetadot = self.state\n",
    "        return np.array([np.cos(theta), np.sin(theta), thetadot], dtype=np.float32)\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "def angle_normalize(x):\n",
    "    return ((x + np.pi) % (2 * np.pi)) - np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "def generate_synthetic_data(env, policy, replay_buffer, num_samples):\n",
    "    \"\"\"\n",
    "    Generate synthetic transitions using the ModelEnv and add them to the SAC replay buffer.\n",
    "\n",
    "    Parameters:\n",
    "    - env: The ModelEnv instance used to simulate transitions.\n",
    "    - policy: The SAC policy used to predict actions.\n",
    "    - replay_buffer: The SAC replay buffer where synthetic data will be stored.\n",
    "    - num_samples: Number of synthetic transitions to generate.\n",
    "    \"\"\"\n",
    "    for _ in range(num_samples):\n",
    "        # Reset the environment to a random initial state\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        # Predict action using the current policy\n",
    "        action = policy.predict(state, deterministic=False)[0]\n",
    "\n",
    "        # Step through the environment\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        # Add synthetic transition to the replay buffer (correct argument order)\n",
    "        replay_buffer.add(state, next_state, action, reward, done, [{}])  # Correct argument order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "# Create the real environment (MountainCarContinuous-v0)\n",
    "real_env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "# Create the planning environment (ModelEnv)\n",
    "model_env = ModelEnv()\n",
    "\n",
    "# Configure logger\n",
    "logger = configure(\"./logs\", [\"stdout\", \"csv\"])\n",
    "\n",
    "# Initialize the SAC agent\n",
    "model = SAC(\n",
    "    \"MlpPolicy\",\n",
    "    real_env,\n",
    "    buffer_size=10000,\n",
    "    # learning_rate=1e-3,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "model.set_logger(logger)  # Ensure logger is set\n",
    "\n",
    "# Training parameters\n",
    "num_real_episodes = 20 # use 40 without synthetic data\n",
    "num_synthetic_samples = 100 \n",
    "steps_per_episode = 200  # Max steps per episode\n",
    "gradient_steps = 2\n",
    "\n",
    "def train():\n",
    "    # Training loop\n",
    "    for episode in range(num_real_episodes):\n",
    "        print(f\"Episode {episode + 1}/{num_real_episodes}\")\n",
    "\n",
    "        total_reward = 0.\n",
    "        # Real environment interaction\n",
    "        state, _ = real_env.reset()\n",
    "\n",
    "        for _ in range(steps_per_episode):\n",
    "            # Predict action using the current policy\n",
    "            action, _ = model.predict(state, deterministic=True)\n",
    "\n",
    "            # Take action in the real environment\n",
    "            next_state, reward, done, truncated, _ = real_env.step(action)\n",
    "            # Store real transition in the replay buffer\n",
    "            model.replay_buffer.add(state, next_state, action, reward, done, [{}])\n",
    "\n",
    "            # Update the agent using real data\n",
    "            model.train(gradient_steps=gradient_steps)  # Specify the number of gradient steps\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "        print(total_reward)\n",
    "        # Generate synthetic data for planning\n",
    "        generate_synthetic_data(model_env, model.policy, model.replay_buffer, num_synthetic_samples)\n",
    "\n",
    "        # Update the agent using synthetic data\n",
    "        for _ in range(num_synthetic_samples):\n",
    "            model.train(gradient_steps=gradient_steps) # Specify the number of gradient steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/20\n",
      "-1616.9456392136676\n",
      "Episode 2/20\n",
      "-1260.8314701933825\n",
      "Episode 3/20\n",
      "-1498.4258610966913\n",
      "Episode 4/20\n",
      "-1004.0343376745576\n",
      "Episode 5/20\n",
      "-1310.1021712788888\n",
      "Episode 6/20\n",
      "-1182.2698668967444\n",
      "Episode 7/20\n",
      "-1269.0745698775866\n",
      "Episode 8/20\n",
      "-948.7601088815923\n",
      "Episode 9/20\n",
      "-3.3820523950600925\n",
      "Episode 10/20\n",
      "-882.1633047755436\n",
      "Episode 11/20\n",
      "-125.80697756636599\n",
      "Episode 12/20\n",
      "-392.51316708245866\n",
      "Episode 13/20\n",
      "-126.38799574354539\n",
      "Episode 14/20\n",
      "-449.9740361074566\n",
      "Episode 15/20\n",
      "-123.92257508164793\n",
      "Episode 16/20\n",
      "-384.0280549167835\n",
      "Episode 17/20\n",
      "-245.66296672527835\n",
      "Episode 18/20\n",
      "-114.83711341060992\n",
      "Episode 19/20\n",
      "-126.67620911377342\n",
      "Episode 20/20\n",
      "-251.446452651077\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward over 100 evaluation episodes: -371.866648247885\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained agent\n",
    "real_env = gym.make(\"Pendulum-v1\", render_mode=\"human\")\n",
    "# real_env = gym.make(\"Pendulum-v1\")\n",
    "total_rewards = []\n",
    "for _ in range(1):  # Evaluate for 10 episodes\n",
    "    state, _ = real_env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = model.predict(state, deterministic=True)\n",
    "        action = np.array(action, dtype=np.float32).reshape(real_env.action_space.shape)\n",
    "        state, reward, terminated, truncated, _ = real_env.step(action)\n",
    "        total_reward += reward\n",
    "        real_env.render()\n",
    "        if terminated or truncated:\n",
    "            done = True\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "print(f\"Average reward over 100 evaluation episodes: {np.mean(total_rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.46e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 7058      |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 0         |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -1.35e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 4785         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 0            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037082315 |\n",
      "|    clip_fraction        | 0.033        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -0.000287    |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 3.76e+03     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00267     |\n",
      "|    std                  | 0.991        |\n",
      "|    value_loss           | 1.07e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -1.28e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 4242         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 1            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032030032 |\n",
      "|    clip_fraction        | 0.0157       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.075        |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 2.81e+03     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00127     |\n",
      "|    std                  | 0.958        |\n",
      "|    value_loss           | 7.22e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -1.27e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 4034         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031586722 |\n",
      "|    clip_fraction        | 0.0213       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.00457      |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 1.83e+03     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00158     |\n",
      "|    std                  | 0.957        |\n",
      "|    value_loss           | 5.54e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -1.26e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3930        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003109281 |\n",
      "|    clip_fraction        | 0.0198      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.00115     |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 2.22e+03    |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00116    |\n",
      "|    std                  | 0.96        |\n",
      "|    value_loss           | 6.91e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -1.28e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3882        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008647452 |\n",
      "|    clip_fraction        | 0.057       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.000684    |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 2.64e+03    |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00503    |\n",
      "|    std                  | 0.994       |\n",
      "|    value_loss           | 6.66e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -1.27e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3845         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060340953 |\n",
      "|    clip_fraction        | 0.0509       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.0003       |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 2.95e+03     |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00305     |\n",
      "|    std                  | 0.973        |\n",
      "|    value_loss           | 7.32e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -1.26e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3784         |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043700757 |\n",
      "|    clip_fraction        | 0.0362       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.00056      |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 1.3e+03      |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.000949    |\n",
      "|    std                  | 0.953        |\n",
      "|    value_loss           | 3.9e+03      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -1.25e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3730        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004336075 |\n",
      "|    clip_fraction        | 0.0421      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.000173    |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 1.46e+03    |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00292    |\n",
      "|    std                  | 0.96        |\n",
      "|    value_loss           | 3.98e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -1.22e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3725         |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043696826 |\n",
      "|    clip_fraction        | 0.0283       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | 0.000194     |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 1.47e+03     |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00102     |\n",
      "|    std                  | 0.939        |\n",
      "|    value_loss           | 3.76e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -1.23e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3730         |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059988378 |\n",
      "|    clip_fraction        | 0.0664       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | 0.000249     |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 833          |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00409     |\n",
      "|    std                  | 0.96         |\n",
      "|    value_loss           | 2.3e+03      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -1.21e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3733        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006161531 |\n",
      "|    clip_fraction        | 0.0463      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 5.41e-05    |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 1.58e+03    |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00379    |\n",
      "|    std                  | 0.916       |\n",
      "|    value_loss           | 4.5e+03     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -1.21e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3739         |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050271917 |\n",
      "|    clip_fraction        | 0.0605       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.35        |\n",
      "|    explained_variance   | 8.37e-05     |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 787          |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00284     |\n",
      "|    std                  | 0.94         |\n",
      "|    value_loss           | 2.15e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -1.19e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3741         |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071012927 |\n",
      "|    clip_fraction        | 0.0508       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 9.02e-05     |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 674          |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00241     |\n",
      "|    std                  | 0.98         |\n",
      "|    value_loss           | 1.89e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -1.18e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3719         |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 8            |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046347114 |\n",
      "|    clip_fraction        | 0.032        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 7.46e-05     |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 589          |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00195     |\n",
      "|    std                  | 0.973        |\n",
      "|    value_loss           | 1.72e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -1.14e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3703        |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005075561 |\n",
      "|    clip_fraction        | 0.048       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 6.99e-05    |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 446         |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00207    |\n",
      "|    std                  | 0.982       |\n",
      "|    value_loss           | 1.69e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -1.14e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3684        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004946238 |\n",
      "|    clip_fraction        | 0.0541      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 7.59e-05    |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 206         |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00058    |\n",
      "|    std                  | 0.961       |\n",
      "|    value_loss           | 761         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -1.14e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3669         |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054534646 |\n",
      "|    clip_fraction        | 0.0527       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | 4.86e-05     |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 374          |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    std                  | 0.929        |\n",
      "|    value_loss           | 1.2e+03      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -1.13e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3648         |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046596453 |\n",
      "|    clip_fraction        | 0.0492       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.35        |\n",
      "|    explained_variance   | 7.49e-05     |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 309          |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00262     |\n",
      "|    std                  | 0.929        |\n",
      "|    value_loss           | 1.14e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -1.13e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3655        |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006905033 |\n",
      "|    clip_fraction        | 0.0396      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.000247    |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 303         |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.000987   |\n",
      "|    std                  | 0.92        |\n",
      "|    value_loss           | 883         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -1.1e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3662        |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005050939 |\n",
      "|    clip_fraction        | 0.0497      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.274       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 130         |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00178    |\n",
      "|    std                  | 0.902       |\n",
      "|    value_loss           | 500         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -1.1e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3669        |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007752604 |\n",
      "|    clip_fraction        | 0.0552      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.6         |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 238         |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00411    |\n",
      "|    std                  | 0.869       |\n",
      "|    value_loss           | 408         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -1.1e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3675        |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005994585 |\n",
      "|    clip_fraction        | 0.0757      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.629       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 244         |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00836    |\n",
      "|    std                  | 0.884       |\n",
      "|    value_loss           | 574         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -1.09e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3680         |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046208273 |\n",
      "|    clip_fraction        | 0.0528       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.3         |\n",
      "|    explained_variance   | 0.69         |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 150          |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00254     |\n",
      "|    std                  | 0.895        |\n",
      "|    value_loss           | 406          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -1.08e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3682         |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046923067 |\n",
      "|    clip_fraction        | 0.0525       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.3         |\n",
      "|    explained_variance   | 0.753        |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 227          |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00383     |\n",
      "|    std                  | 0.889        |\n",
      "|    value_loss           | 421          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -1.07e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3682         |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049509513 |\n",
      "|    clip_fraction        | 0.0518       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | 0.807        |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 94.7         |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00173     |\n",
      "|    std                  | 0.866        |\n",
      "|    value_loss           | 249          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -1.07e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3683         |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064894743 |\n",
      "|    clip_fraction        | 0.065        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.26        |\n",
      "|    explained_variance   | 0.745        |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 104          |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.00412     |\n",
      "|    std                  | 0.848        |\n",
      "|    value_loss           | 175          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -1.04e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3686        |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004961339 |\n",
      "|    clip_fraction        | 0.0491      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.793       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 131         |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00272    |\n",
      "|    std                  | 0.862       |\n",
      "|    value_loss           | 298         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -1.02e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3688        |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007448473 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.757       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 99.7        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00525    |\n",
      "|    std                  | 0.853       |\n",
      "|    value_loss           | 169         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -1.01e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3680         |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067663537 |\n",
      "|    clip_fraction        | 0.0779       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.26        |\n",
      "|    explained_variance   | 0.781        |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 124          |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00542     |\n",
      "|    std                  | 0.844        |\n",
      "|    value_loss           | 245          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -971        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3683        |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008057282 |\n",
      "|    clip_fraction        | 0.0855      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.815       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 107         |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00588    |\n",
      "|    std                  | 0.839       |\n",
      "|    value_loss           | 268         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -954        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3679        |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013243303 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 65.2        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00516    |\n",
      "|    std                  | 0.819       |\n",
      "|    value_loss           | 172         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -913       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3683       |\n",
      "|    iterations           | 33         |\n",
      "|    time_elapsed         | 18         |\n",
      "|    total_timesteps      | 67584      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01096482 |\n",
      "|    clip_fraction        | 0.118      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.21      |\n",
      "|    explained_variance   | 0.872      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 86.6       |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | -0.00902   |\n",
      "|    std                  | 0.816      |\n",
      "|    value_loss           | 232        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -884        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3679        |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015501065 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 38.8        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00445    |\n",
      "|    std                  | 0.801       |\n",
      "|    value_loss           | 95.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -852        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3681        |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012718903 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 68.7        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00503    |\n",
      "|    std                  | 0.79        |\n",
      "|    value_loss           | 169         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -822        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3682        |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010383695 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 161         |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00724    |\n",
      "|    std                  | 0.784       |\n",
      "|    value_loss           | 246         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -766        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3685        |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014373627 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 135         |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0151     |\n",
      "|    std                  | 0.779       |\n",
      "|    value_loss           | 355         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -712        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3687        |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018828653 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 92.2        |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.014      |\n",
      "|    std                  | 0.786       |\n",
      "|    value_loss           | 283         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -645        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3688        |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026099041 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 79.6        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    std                  | 0.748       |\n",
      "|    value_loss           | 199         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -585        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3691        |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012210157 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.939       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 86.9        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00389    |\n",
      "|    std                  | 0.72        |\n",
      "|    value_loss           | 155         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -532        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3692        |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025682095 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 56.9        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | 0.00303     |\n",
      "|    std                  | 0.699       |\n",
      "|    value_loss           | 150         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -473       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3689       |\n",
      "|    iterations           | 42         |\n",
      "|    time_elapsed         | 23         |\n",
      "|    total_timesteps      | 86016      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03690408 |\n",
      "|    clip_fraction        | 0.155      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.04      |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 45.2       |\n",
      "|    n_updates            | 410        |\n",
      "|    policy_gradient_loss | 0.000912   |\n",
      "|    std                  | 0.682      |\n",
      "|    value_loss           | 107        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -425        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3688        |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014770495 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 34.1        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.00657    |\n",
      "|    std                  | 0.662       |\n",
      "|    value_loss           | 166         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -381       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3691       |\n",
      "|    iterations           | 44         |\n",
      "|    time_elapsed         | 24         |\n",
      "|    total_timesteps      | 90112      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02247091 |\n",
      "|    clip_fraction        | 0.183      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.999     |\n",
      "|    explained_variance   | 0.959      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 46.4       |\n",
      "|    n_updates            | 430        |\n",
      "|    policy_gradient_loss | -0.0125    |\n",
      "|    std                  | 0.655      |\n",
      "|    value_loss           | 136        |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -325         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3694         |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076862285 |\n",
      "|    clip_fraction        | 0.105        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.998       |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 78.9         |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.00539     |\n",
      "|    std                  | 0.654        |\n",
      "|    value_loss           | 162          |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -299       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3696       |\n",
      "|    iterations           | 46         |\n",
      "|    time_elapsed         | 25         |\n",
      "|    total_timesteps      | 94208      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07757145 |\n",
      "|    clip_fraction        | 0.31       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.999     |\n",
      "|    explained_variance   | 0.959      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 3.78       |\n",
      "|    n_updates            | 450        |\n",
      "|    policy_gradient_loss | 0.017      |\n",
      "|    std                  | 0.664      |\n",
      "|    value_loss           | 16.4       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -275        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3698        |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006663508 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 125         |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.008      |\n",
      "|    std                  | 0.675       |\n",
      "|    value_loss           | 378         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -264        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3686        |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007940882 |\n",
      "|    clip_fraction        | 0.0766      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 8.48        |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.00424    |\n",
      "|    std                  | 0.668       |\n",
      "|    value_loss           | 81          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -270         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3684         |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041877707 |\n",
      "|    clip_fraction        | 0.0395       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 0.939        |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 94           |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.00154     |\n",
      "|    std                  | 0.667        |\n",
      "|    value_loss           | 94.6         |\n",
      "------------------------------------------\n",
      "Episode 1: Total Reward = -255.03970309380904\n",
      "Episode 2: Total Reward = -3.7447113548955677\n",
      "Episode 3: Total Reward = -251.38041177896332\n",
      "Episode 4: Total Reward = -133.75819803311404\n",
      "Episode 5: Total Reward = -130.13421345943766\n",
      "Episode 6: Total Reward = -256.92986295819054\n",
      "Episode 7: Total Reward = -244.829194010963\n",
      "Episode 8: Total Reward = -124.11631842817222\n",
      "Episode 9: Total Reward = -123.75249471025484\n",
      "Episode 10: Total Reward = -236.24879895601265\n",
      "Episode 11: Total Reward = -251.7063039464975\n",
      "Episode 12: Total Reward = -394.03049961106973\n",
      "Episode 13: Total Reward = -126.1399146052846\n",
      "Episode 14: Total Reward = -600.3124232926098\n",
      "Episode 15: Total Reward = -3.753806884553985\n",
      "Episode 16: Total Reward = -125.77935839382306\n",
      "Episode 17: Total Reward = -358.8178567466354\n",
      "Episode 18: Total Reward = -250.47425578502578\n",
      "Episode 19: Total Reward = -127.93603142421\n",
      "Episode 20: Total Reward = -247.11610783306702\n",
      "Episode 21: Total Reward = -246.48782998343538\n",
      "Episode 22: Total Reward = -4.023734808284757\n",
      "Episode 23: Total Reward = -4.003113524238134\n",
      "Episode 24: Total Reward = -127.81339727580949\n",
      "Episode 25: Total Reward = -131.27296897010245\n",
      "Episode 26: Total Reward = -350.69014641421956\n",
      "Episode 27: Total Reward = -129.99636121185907\n",
      "Episode 28: Total Reward = -3.904943599185794\n",
      "Episode 29: Total Reward = -130.02892700506123\n",
      "Episode 30: Total Reward = -4.295130137846018\n",
      "Episode 31: Total Reward = -4.014070751803781\n",
      "Episode 32: Total Reward = -263.8273110770793\n",
      "Episode 33: Total Reward = -273.731796447858\n",
      "Episode 34: Total Reward = -255.14458129064946\n",
      "Episode 35: Total Reward = -395.2876281126495\n",
      "Episode 36: Total Reward = -130.0953175267931\n",
      "Episode 37: Total Reward = -4.103413523191227\n",
      "Episode 38: Total Reward = -4.163782138717297\n",
      "Episode 39: Total Reward = -128.77260466253313\n",
      "Episode 40: Total Reward = -251.58054395133593\n",
      "Episode 41: Total Reward = -252.7798846541936\n",
      "Episode 42: Total Reward = -379.0568764215993\n",
      "Episode 43: Total Reward = -272.7719214998628\n",
      "Episode 44: Total Reward = -248.7282890896346\n",
      "Episode 45: Total Reward = -259.4507497117155\n",
      "Episode 46: Total Reward = -128.5425045370357\n",
      "Episode 47: Total Reward = -3.817049284544586\n",
      "Episode 48: Total Reward = -130.70186958514466\n",
      "Episode 49: Total Reward = -3.860330742113972\n",
      "Episode 50: Total Reward = -249.33173665941734\n",
      "Episode 51: Total Reward = -250.7839929222919\n",
      "Episode 52: Total Reward = -480.0365888882305\n",
      "Episode 53: Total Reward = -481.70578932387457\n",
      "Episode 54: Total Reward = -366.9002360454028\n",
      "Episode 55: Total Reward = -128.81459470616596\n",
      "Episode 56: Total Reward = -358.604222094682\n",
      "Episode 57: Total Reward = -272.29332683649153\n",
      "Episode 58: Total Reward = -269.22196163520965\n",
      "Episode 59: Total Reward = -125.23664731295268\n",
      "Episode 60: Total Reward = -126.01151058555831\n",
      "Episode 61: Total Reward = -248.870199565737\n",
      "Episode 62: Total Reward = -129.93816990628432\n",
      "Episode 63: Total Reward = -129.36404528101025\n",
      "Episode 64: Total Reward = -243.82462875976296\n",
      "Episode 65: Total Reward = -128.52314682126138\n",
      "Episode 66: Total Reward = -125.54088197533937\n",
      "Episode 67: Total Reward = -4.992486999226791\n",
      "Episode 68: Total Reward = -253.03131239713588\n",
      "Episode 69: Total Reward = -254.10517532492963\n",
      "Episode 70: Total Reward = -127.42538067573618\n",
      "Episode 71: Total Reward = -249.49370526330566\n",
      "Episode 72: Total Reward = -279.1772718203636\n",
      "Episode 73: Total Reward = -392.52471350614115\n",
      "Episode 74: Total Reward = -408.62768363243816\n",
      "Episode 75: Total Reward = -133.42527284763588\n",
      "Episode 76: Total Reward = -379.7576633280437\n",
      "Episode 77: Total Reward = -4.54298826208466\n",
      "Episode 78: Total Reward = -380.2726566975392\n",
      "Episode 79: Total Reward = -4.325508211253786\n",
      "Episode 80: Total Reward = -252.936046690177\n",
      "Episode 81: Total Reward = -252.74869442033196\n",
      "Episode 82: Total Reward = -129.39336181312274\n",
      "Episode 83: Total Reward = -258.45607359590144\n",
      "Episode 84: Total Reward = -128.50083918802918\n",
      "Episode 85: Total Reward = -126.44340842568735\n",
      "Episode 86: Total Reward = -125.55929317076465\n",
      "Episode 87: Total Reward = -128.3878506289534\n",
      "Episode 88: Total Reward = -3.6292526983067743\n",
      "Episode 89: Total Reward = -496.01834037266167\n",
      "Episode 90: Total Reward = -123.25741995196636\n",
      "Episode 91: Total Reward = -122.15704885804246\n",
      "Episode 92: Total Reward = -249.77421682896204\n",
      "Episode 93: Total Reward = -358.49574663400836\n",
      "Episode 94: Total Reward = -243.45433325006007\n",
      "Episode 95: Total Reward = -7.169146328069648\n",
      "Episode 96: Total Reward = -130.55279231992083\n",
      "Episode 97: Total Reward = -4.173829523664031\n",
      "Episode 98: Total Reward = -122.36460442387663\n",
      "Episode 99: Total Reward = -242.75532342721084\n",
      "Episode 100: Total Reward = -122.86164410700586\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "def main():\n",
    "    # Create the environment\n",
    "    env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "    # Initialize the PPO model\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",  # Use a Multi-Layer Perceptron policy\n",
    "        env,\n",
    "        learning_rate=1e-3,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.learn(total_timesteps=100000)\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save(\"ppo_mountaincar_continuous\")\n",
    "\n",
    "    # Load the trained model\n",
    "    model = PPO.load(\"ppo_mountaincar_continuous\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    episodes = 100\n",
    "    for ep in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done or truncated:\n",
    "                break\n",
    "        print(f\"Episode {ep + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alucantonio/data_enhanced_simulation/blob/master/12_MBRL.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Based Reinforcement Learning (MBRL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-based reinforcement learning uses a model of the dynamics of the environment to\n",
    "guide the agent's learning and decision-making. This approach differs from model-free\n",
    "RL, which learns a policy of value function directly from interaction with the environment.\n",
    "\n",
    "The model of the environment is expressed in terms of the environment transition dynamics, $s_{t+1} = f(s_t,\n",
    "a_t)$ (think of a deterministic environment), and the reward function $R(s,a)$. The\n",
    "agent either uses a predefined model or learns one through interactions with the environment. \n",
    "\n",
    "The agent uses the model to simulate trajectories, evaluate potential actions, and\n",
    "**plan** future behavior. Then, the agent optimizes its policy or value function based\n",
    "on the predicted outcomes and periodically interacts with the actual environment to\n",
    "improve the model and correct inaccuracies in its predictions.\n",
    "\n",
    "Advantages of MBRL:\n",
    "\n",
    "- Sample efficiency: by learning and simulating within the model, the agent can reduce the need for extensive interactions with the real environment, making it particularly useful when interactions are costly or limited.\n",
    "- Generalization: a well-learned model can generalize to unseen scenarios and adapt more easily to changes.\n",
    "- Interpretability: the explicit model provides insights into the environment’s dynamics.\n",
    "\n",
    "Challenges:\n",
    "\n",
    "- Model accuracy: errors in the learned model can lead to suboptimal or unsafe decisions (a problem known as “model bias”).\n",
    "- Complexity: learning and maintaining a reliable model can be computationally expensive, especially in high-dimensional or stochastic environments.\n",
    "- Trade-off with exploration: balancing exploration of the real environment and reliance on the model requires careful design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dyna-Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dyna-Q algorithm combines ideas from both model-free RL and model-based planning to\n",
    "increase learning efficiency. Specifically, it employs both real-world experience and simulated experience derived\n",
    "from a learned model of the environment.\n",
    "\n",
    "Key components:\n",
    "1.\tValue function (Q-function)\n",
    "2.\tModel of the environment: it consists of a transition model and a reward model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo-code of the algorithm:\n",
    "\n",
    "```pseudo\n",
    "Initialize Q(s, a) arbitrarily\n",
    "Initialize the model (empty initially)\n",
    "\n",
    "Repeat (for each episode or until convergence):\n",
    "    1. Observe the current state s\n",
    "    2. Select action a using an epsilon-greedy policy\n",
    "    3. Execute a, observe reward r and next state s'\n",
    "    4. Update Q(s, a) using the Q-learning update rule\n",
    "    5. Update the model with (s, a, r, s')\n",
    "    6. For n planning steps:\n",
    "        a. Randomly sample (s_sim, a_sim) from the model\n",
    "        b. Predict r_sim, s'_sim using the model\n",
    "        c. Update Q(s_sim, a_sim) using the Q-learning update rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAC-Dyna-Q-like algorithm\n",
    "\n",
    "We can combine an actor-critic model (SAC - Soft Actor Critic) with a planning component\n",
    "based on the model to create a Dyna-Q-like algorithm.\n",
    "\n",
    "Here is the pseudo-code:\n",
    "\n",
    "1. Initialize environment $ \\text{env} $.\n",
    "2. Initialize SAC agent and replay buffer $ \\mathcal{D} $.\n",
    "3. Set $ N_\\text{real\\_episodes}, N_\\text{synthetic\\_samples} $.\n",
    "\n",
    "For $ \\text{episode} $ in $ N_\\text{real\\_episodes} $:\n",
    "1. **Collect real data:**\n",
    "   - Reset environment.\n",
    "   - For each step in the environment:\n",
    "     1. Select action $a_t$ according to the agent's policy\n",
    "     2. Execute $ a_t $, observe $ s_{t+1}, r_t, \\text{done} $.\n",
    "     3. Store $ (s_t, a_t, r_t, s_{t+1}, \\text{done}) $ in $ \\mathcal{D} $.\n",
    "\n",
    "2. **Model-free SAC update:**\n",
    "   - Sample $ (s, a, r, s', \\text{done}) $ from $ \\mathcal{D} $.\n",
    "   - Update value and policy networks using SAC objectives.\n",
    "\n",
    "3. **Generate synthetic data:**\n",
    "   - For $ i $ in $ N_\\text{synthetic\\_samples} $:\n",
    "     1. Sample $ s $ from $ \\mathcal{D} $ or the observation space.\n",
    "     2. Predict action according to the agent's policy\n",
    "     3. Simulate $ s', r $ using the model\n",
    "     4. Store $ (s, a, r, s', \\text{done}) $ in $ \\mathcal{D} $.\n",
    "\n",
    "4. **SAC update with synthetic data:**\n",
    "   - Repeat step 2 with synthetic data added to $ \\mathcal{D} $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the `Pendulum` environment with MBRL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will implement the SAC-Dyna-Q-like algorithm described above to\n",
    "solve the `Pendulum` environment of `gymnasium`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use Symbolic Regression (see notebook n. 11) to discover the equation for the\n",
    "   evolution of the angular velocity:\n",
    "\n",
    "   $\\theta_{t+1} = f(\\theta_t, a_t)$\n",
    "\n",
    "   where $a_t$ is the action (torque). Compare it with the true dynamics:\n",
    "\n",
    "   $\\theta_{t+1} = \\theta_t + \\Delta t\\frac{3g}{2l}\\sin \\theta_t + \\Delta\n",
    "   t\\frac{3}{ml^2}a_t$\n",
    "   \n",
    "   where $g$ is the gravity acceleration, $l$ is the length of the pendulum, $m$ is the\n",
    "   mass of the pendulum and $\\Delta t$ is the time-step of the simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Implement a `gymnasium` enviroment based on the learned model. Complete the `step`\n",
    "   function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class ModelEnv(gym.Env):\n",
    "\n",
    "    def __init__(self, render_mode: Optional[str] = None, g=10.0):\n",
    "        self.max_speed = 8\n",
    "        self.max_torque = 2.0\n",
    "        self.dt = 0.05\n",
    "        self.g = g\n",
    "        self.m = 1.0\n",
    "        self.l = 1.0\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.screen_dim = 500\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        self.isopen = True\n",
    "\n",
    "        high = np.array([1.0, 1.0, self.max_speed], dtype=np.float32)\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-self.max_torque, high=self.max_torque, shape=(1,), dtype=np.float32\n",
    "        )\n",
    "        self.observation_space = spaces.Box(low=-high, high=high, dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        th, thdot = self.state  # th := theta\n",
    "\n",
    "        dt = self.dt\n",
    "\n",
    "        action = np.clip(action, -self.max_torque, self.max_torque)[0]\n",
    "        costs = angle_normalize(th) ** 2 + 0.1 * thdot**2 + 0.001 * (action**2)\n",
    "\n",
    "        # UPDATE ANGULAR VELOCITY HERE based on the equation found via symbolic regression\n",
    "        # newthdot = ...\n",
    "\n",
    "        newthdot = np.clip(newthdot, -self.max_speed, self.max_speed)\n",
    "        newth = th + newthdot * dt\n",
    "\n",
    "        self.state = np.array([newth, newthdot])\n",
    "\n",
    "        return self._get_obs(), -costs, False, False, {}\n",
    "\n",
    "    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "        super().reset(seed=seed)\n",
    "        high = np.array([np.pi, 1.])\n",
    "        low = -high\n",
    "        self.state = self.np_random.uniform(low=low, high=high)\n",
    "\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        theta, thetadot = self.state\n",
    "        return np.array([np.cos(theta), np.sin(theta), thetadot], dtype=np.float32)\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "def angle_normalize(x):\n",
    "    return ((x + np.pi) % (2 * np.pi)) - np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Complete the function `generate_synthetic_data` that uses an instance of `ModelEnv`\n",
    "   generate samples according to the policy and adds them to\n",
    "   the SAC replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "def generate_synthetic_data(env, policy, replay_buffer, num_samples):\n",
    "    \"\"\"\n",
    "    Generate synthetic transitions using the ModelEnv and add them to the SAC replay buffer.\n",
    "\n",
    "    Parameters:\n",
    "    - env: The ModelEnv instance used to simulate transitions.\n",
    "    - policy: The SAC policy used to predict actions.\n",
    "    - replay_buffer: The SAC replay buffer where synthetic data will be stored.\n",
    "    - num_samples: Number of synthetic transitions to generate.\n",
    "    \"\"\"\n",
    "    for _ in range(num_samples):\n",
    "        # Reset the environment to a random initial state\n",
    "        # ...\n",
    "\n",
    "        # Predict action using the current policy\n",
    "        # ...\n",
    "\n",
    "        # Step through the environment\n",
    "        # ...\n",
    "\n",
    "        if terminated or truncated:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        # Add synthetic transition to the replay buffer\n",
    "        replay_buffer.add(state, next_state, action, reward, done, [{}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution:\n",
    "\n",
    "from stable_baselines3 import SAC\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "def generate_synthetic_data(env, policy, replay_buffer, num_samples):\n",
    "    \"\"\"\n",
    "    Generate synthetic transitions using the ModelEnv and add them to the SAC replay buffer.\n",
    "\n",
    "    Parameters:\n",
    "    - env: The ModelEnv instance used to simulate transitions.\n",
    "    - policy: The SAC policy used to predict actions.\n",
    "    - replay_buffer: The SAC replay buffer where synthetic data will be stored.\n",
    "    - num_samples: Number of synthetic transitions to generate.\n",
    "    \"\"\"\n",
    "    for _ in range(num_samples):\n",
    "        # Reset the environment to a random initial state\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        # Predict action using the current policy\n",
    "        action = policy.predict(state, deterministic=False)[0]\n",
    "\n",
    "        # Step through the environment\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        # Add synthetic transition to the replay buffer\n",
    "        replay_buffer.add(state, next_state, action, reward, done, [{}])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Complete the function `train` that implements the training loop of the\n",
    "   SAC-Dyna-Q-like algorithm. Check the [docs](https://stable-baselines3.readthedocs.io/en/master/modules/sac.html) of the SAC class for methods needed for\n",
    "   training the policy. Run the training on the `Pendulum-v1` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "# Create the real environment\n",
    "real_env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "# Create the planning environment\n",
    "model_env = ModelEnv()\n",
    "\n",
    "# Configure logger\n",
    "logger = configure(\"./logs\", [\"stdout\", \"csv\"])\n",
    "\n",
    "# Initialize the SAC agent\n",
    "# model = ...\n",
    "\n",
    "model.set_logger(logger)\n",
    "\n",
    "# Training parameters\n",
    "num_real_episodes = 20\n",
    "steps_per_episode = 200\n",
    "# ...\n",
    "# ...\n",
    "\n",
    "def train():\n",
    "    for episode in range(num_real_episodes):\n",
    "        print(f\"Episode {episode + 1}/{num_real_episodes}\")\n",
    "\n",
    "        # Real environment interaction\n",
    "        state, _ = real_env.reset()\n",
    "\n",
    "        for _ in range(steps_per_episode):\n",
    "            # Predict action using the current policy\n",
    "            # ...\n",
    "\n",
    "            # Take action in the real environment\n",
    "            # ...\n",
    "\n",
    "            # Store real transition in the replay buffer\n",
    "            # ...\n",
    "\n",
    "            # Update the agent using real data\n",
    "            # ...\n",
    "\n",
    "            state = next_state\n",
    "            \n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "        # Generate synthetic data for planning\n",
    "        generate_synthetic_data(model_env, model.policy, model.replay_buffer, num_synthetic_samples)\n",
    "\n",
    "        # Update the agent using synthetic data\n",
    "        # ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution:\n",
    "\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "# Create the real environment\n",
    "real_env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "# Create the planning environment\n",
    "model_env = ModelEnv()\n",
    "\n",
    "# Configure logger\n",
    "logger = configure(\"./logs\", [\"stdout\", \"csv\"])\n",
    "\n",
    "# Initialize the SAC agent\n",
    "model = SAC(\n",
    "    \"MlpPolicy\",\n",
    "    real_env,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "model.set_logger(logger)\n",
    "\n",
    "# Training parameters\n",
    "num_real_episodes = 20\n",
    "num_synthetic_samples = 100 \n",
    "steps_per_episode = 200\n",
    "gradient_steps = 2\n",
    "\n",
    "def train():\n",
    "    for episode in range(num_real_episodes):\n",
    "        print(f\"Episode {episode + 1}/{num_real_episodes}\")\n",
    "\n",
    "        total_reward = 0.\n",
    "        # Real environment interaction\n",
    "        state, _ = real_env.reset()\n",
    "\n",
    "        for _ in range(steps_per_episode):\n",
    "            # Predict action using the current policy\n",
    "            action, _ = model.predict(state, deterministic=False)\n",
    "\n",
    "            # Take action in the real environment\n",
    "            next_state, reward, done, truncated, _ = real_env.step(action)\n",
    "\n",
    "            # Store real transition in the replay buffer\n",
    "            model.replay_buffer.add(state, next_state, action, reward, done, [{}])\n",
    "\n",
    "            # Update the agent using real data\n",
    "            model.train(gradient_steps=gradient_steps)\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "        print(total_reward)\n",
    "        # Generate synthetic data for planning\n",
    "        generate_synthetic_data(model_env, model.policy, model.replay_buffer, num_synthetic_samples)\n",
    "\n",
    "        # Update the agent using synthetic data\n",
    "        for _ in range(num_synthetic_samples):\n",
    "            model.train(gradient_steps=gradient_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Play 100 episodes using the trained SAC policy (set `deterministic=True` when using\n",
    "   the `predict` method) and evaluate the average reward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution:\n",
    "\n",
    "# Evaluate the trained agent\n",
    "# real_env = gym.make(\"Pendulum-v1\", render_mode=\"human\")\n",
    "real_env = gym.make(\"Pendulum-v1\")\n",
    "total_rewards = []\n",
    "for _ in range(100):  # Evaluate for 10 episodes\n",
    "    state, _ = real_env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = model.predict(state, deterministic=True)\n",
    "        action = np.array(action, dtype=np.float32).reshape(real_env.action_space.shape)\n",
    "        state, reward, terminated, truncated, _ = real_env.step(action)\n",
    "        total_reward += reward\n",
    "        # real_env.render()\n",
    "        if terminated or truncated:\n",
    "            done = True\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "print(f\"Average reward over 100 evaluation episodes: {np.mean(total_rewards)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Train a PPO agent and compare the average reward over 100 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution:\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "# Initialize the PPO model\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",  # Use a Multi-Layer Perceptron policy\n",
    "    env,\n",
    "    learning_rate=7e-4,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=200000)\n",
    "\n",
    "# Evaluate the model\n",
    "episodes = 100\n",
    "total_rewards = []\n",
    "for ep in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done or truncated:\n",
    "            break\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "print(f\"Average reward over 100 evaluation episodes: {np.mean(total_rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

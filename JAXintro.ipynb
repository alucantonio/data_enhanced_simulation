{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "455ea6a9",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alucantonio/data_enhanced_simulation/blob/master/JAXintro.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39739727-e0e6-464a-879c-cee76d6dee18",
   "metadata": {},
   "source": [
    "# Introduction to JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b201bc50-327b-4676-bb98-b85319fa53f3",
   "metadata": {},
   "source": [
    "_JAX is a Python library for accelerator-oriented array computation and program transformation, designed for high-performance numerical computing and large-scale machine learning._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f50303-7c66-47d6-b74c-fa5fb778ecb8",
   "metadata": {},
   "source": [
    "Additional resources:\n",
    "- Getting started with JAX (notebooks + videos): https://github.com/gordicaleksa/get-started-with-JAX\n",
    "- JAX tutorials: https://jax.readthedocs.io/en/latest/tutorials.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59662a41-f97b-4f3b-a91c-35d9622bbb5c",
   "metadata": {},
   "source": [
    "If you want to run this notebook locally, follow the instructions [here](https://jax.readthedocs.io/en/latest/installation.html). For full GPU support, use JAX on a Linux platform with NVIDIA GPU. Otherwise, use Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d0b43d-ba4b-4ae9-bbe3-2a06c2521959",
   "metadata": {},
   "source": [
    "## Basic concepts: JAX vs NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7cbea8-237c-49ef-9cb7-35924177e0f1",
   "metadata": {},
   "source": [
    "JAX's syntax is similar to NumPy's, so that many JAX functions can be used as drop-in replacement to NumPy's functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dca27c-b6b4-498b-a785-df2460c57a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for this lesson\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "from jax import random, device_put, jit, grad, vmap, make_jaxpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d936a9f-76c9-459d-bdd0-e64322dd8977",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_np = jnp.linspace(0, 10, 1000)\n",
    "y_np = 2 * jnp.sin(x_np) * jnp.cos(x_np)\n",
    "plt.plot(x_np, y_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ce14bb-3ffa-46ab-914b-4d1a9237e253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent NumPy code\n",
    "x_np = np.linspace(0, 10, 1000)\n",
    "y_np = 2 * np.sin(x_np) * np.cos(x_np)\n",
    "plt.plot(x_np, y_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78999cac-f481-40b6-83a1-f954ee6572e4",
   "metadata": {},
   "source": [
    "Differently from NumPy, JAX arrays are **immutable**. This is because JAX requires that programs are *pure functions*, to simplify analysis and perform better optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75a3b52-15d9-4045-b888-b469f881f4ff",
   "metadata": {},
   "source": [
    "When a *pure function* is called with some given arguments, it will always return the same result, and cannot be affected by any mutable state or other side effects (we will talk more about this later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9509688-1003-41db-8872-80c7e1dc1c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 10\n",
    "index = 0\n",
    "value = 23\n",
    "\n",
    "# In NumPy arrays are mutable\n",
    "x = np.arange(size)\n",
    "print(x)\n",
    "x[index] = value\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c354b77-51fe-40c3-ad79-508cf4d015db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In JAX we have to deal with immutable arrays\n",
    "x = jnp.arange(size)\n",
    "print(x)\n",
    "x[index] = value\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ac51fb-f94f-4840-bf34-559e5d53e6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution/workaround:\n",
    "y = x.at[index].set(value)\n",
    "print(x)\n",
    "print(y)\n",
    "# The updated array is returned as a new array and the original array is not modified by the update.\n",
    "# In some cases, JAX can optimize this and avoid copying (see jit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cb8f6d-0282-4845-a485-3e5ec69afcc6",
   "metadata": {},
   "source": [
    "JAX is accelerator (hardware) agnostic. Same code runs everywhere!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ab3898-904c-487f-b503-e961cb4203f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the default device used for JAX computations\n",
    "default_device = jax.devices()[0]\n",
    "print(\"Default device:\", default_device)\n",
    "\n",
    "# Print the default backend used for JAX computations\n",
    "default_backend = jax.default_backend()\n",
    "print(\"Default backend:\", default_backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9757ff49-cf87-45e9-9071-19066328fb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 3000\n",
    "\n",
    "key = random.key(42)\n",
    "\n",
    "# Data is automatically pushed to the accelerator! (DeviceArray structure)\n",
    "# NOTE: by default, JAX arrays are float32, while NumPy arrays are float64.\n",
    "x_jnp = random.normal(key, (size, size), dtype=jnp.float32)\n",
    "x_np = np.random.normal(size=(size, size)).astype(np.float32)\n",
    "\n",
    "%timeit jnp.dot(x_jnp, x_jnp.T).block_until_ready()  # 1) on GPU - fast\n",
    "%timeit np.dot(x_np, x_np.T)  # 2) on CPU - slow (NumPy only works with CPUs)\n",
    "%timeit jnp.dot(x_np, x_np.T).block_until_ready()  # 3) on GPU with transfer overhead\n",
    "\n",
    "x_np_device = device_put(x_np)  # push NumPy explicitly to GPU\n",
    "%timeit jnp.dot(x_np_device, x_np_device.T).block_until_ready()  # same as 1)\n",
    "\n",
    "# Note 1: I'm using GPU as a synonym for accelerator. \n",
    "# In reality, especially in Colab, this can also be a TPU, etc.\n",
    "\n",
    "# Note 2: block_until_ready() -> asynchronous dispatch (https://jax.readthedocs.io/en/latest/async_dispatch.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1005c548-951d-4843-be19-ae9bf9f4054d",
   "metadata": {},
   "source": [
    "Due to JAX's accelerator agnostic approach, out-of-bounds indexing results in errors, rather than exceptions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28916237-9db9-4136-8529-11f0d304be6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy behavior\n",
    "\n",
    "try:\n",
    "  np.arange(10)[11]\n",
    "except Exception as e:\n",
    "    print(\"Exception {}\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e87877-557e-42f2-aece-4eda8603772e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JAX behavior\n",
    "# 1) updates at out-of-bounds indices are skipped\n",
    "# 2) retrievals result in index being clamped\n",
    "# in general there are currently some bugs so just consider the behavior undefined!\n",
    "\n",
    "print(jnp.arange(10).at[11].add(23))  # example of 1)\n",
    "print(jnp.arange(10)[11])  # example of 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1056b5ab-fb92-4c5d-be04-2b610ce4a296",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8bd325-c90a-4e6b-9978-d3b1638b4c2e",
   "metadata": {},
   "source": [
    "## `vmap`: automatic vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fba4c2e-83f2-4a76-894c-9d7ed33962d0",
   "metadata": {},
   "source": [
    "In NN and numerical computations we frequently write functions that operate on batches of data (i.e. multiple samples of some features). `vmap` helps to generate a _vectorized_ implementation of a function automatically, without going through manual, error-prone and inefficient vectorization. You can then write functions operating on single samples and then vectorize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90b289f-cb9e-4f94-9439-5b81e3d0f5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = random.normal(key, (150, 100))  # e.g. weights of a linear NN layer\n",
    "batched_x = random.normal(key, (10, 100))  # e.g. a batch of 10 flattened images\n",
    "\n",
    "# Function operating on a single sample\n",
    "def apply_matrix(x):\n",
    "    return jnp.dot(W, x)  # (150, 100) * (100, 1) -> (150, 1) (1 sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d7f4ae-5102-46e8-959a-39e3c7d38d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naively_batched_apply_matrix(batched_x):\n",
    "    return jnp.stack([apply_matrix(x) for x in batched_x]) # stack results in a column vector\n",
    "\n",
    "print('Naively batched')\n",
    "%timeit naively_batched_apply_matrix(batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d1b308-6900-4f63-b4b2-65057d3748af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(naively_batched_apply_matrix(batched_x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fb0395-ae52-4a6d-9dd6-2ef13ecaec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_apply_matrix(batched_x):\n",
    "    return jnp.dot(batched_x, W.T)  # (10, 100) * (100, 150) -> (10, 150)\n",
    "\n",
    "print('Manually batched')\n",
    "%timeit batched_apply_matrix(batched_x).block_until_ready()\n",
    "# We can make it even faster with jit... See below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a338633-128e-4346-b5bc-032ce3ae60bc",
   "metadata": {},
   "source": [
    "Use the parameters `in_axes` and `out_axes` to specify which input array axes to map over and where the mapped axis should appear in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90105f03-af50-47d1-8637-a69124928205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: implementing matrix-matrix product by vectorizing the dot product\n",
    "vv = lambda x, y: jnp.vdot(x, y)  #  ([a], [a]) -> []\n",
    "mv = vmap(vv, (0, None), 0)      #  ([b,a], [a]) -> [b]      (b is the mapped axis, i.e. axis 0 of first input)\n",
    "mm = vmap(mv, (None, 1), 1)      #  ([b,a], [a,c]) -> [b,c]  (c is the mapped axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fa6960-43ff-4a36-b5e3-cd223d3b4f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.ones((2,2))\n",
    "y = 5.*jnp.ones((2,1))\n",
    "mv(x,y), mm(x,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54103d26-0925-483c-99a0-e5298068bc5c",
   "metadata": {},
   "source": [
    "## `grad`: automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e972e624-3941-448b-9a24-9491822299a9",
   "metadata": {},
   "source": [
    "The `grad` transformation differentiates _scalar-valued_ functions (by default with resepct to the first argument), using _automatic differentiation_ (_not_ numeric/symbolic, see here https://www.youtube.com/watch?v=wG_nF1awSSY for a nice explanation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d71b1f-3396-43c6-a891-d7f3b18b5cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First example (automatic diff)\n",
    "\n",
    "def loss(x): \n",
    "    return jnp.sum(x**2)\n",
    "\n",
    "x = jnp.arange(3.)\n",
    "\n",
    "# By default grad calculates the derivative of a fn w.r.t. 1st parameter!\n",
    "grad_loss = grad(loss)\n",
    "\n",
    "print(grad_loss(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8da25d2-f21b-43a6-8c4e-2b5a983073d3",
   "metadata": {},
   "source": [
    "We can compose `grad` functions to evaluate higher-order derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ff4bde-1a35-4d43-a4c8-27204d5c7ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x**3 + 2*x**2 - 3*x + 1\n",
    "\n",
    "dfdx = jax.grad(f)\n",
    "d2fdx = jax.grad(dfdx)\n",
    "d3fdx = jax.grad(d2fdx)\n",
    "d4fdx = jax.grad(d3fdx)\n",
    "print(dfdx(1.))\n",
    "print(d2fdx(1.))\n",
    "print(d3fdx(1.))\n",
    "print(d4fdx(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc87e71-c446-4629-aca6-bc1a4ac6b381",
   "metadata": {},
   "source": [
    "We can also use JAX's autodiff to compute **Jacobians** and **Hessians**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7933eca5-6ccb-402e-be9f-c11c55234bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jacfwd, jacrev\n",
    "\n",
    "f = lambda x, y: x**2 + y**2\n",
    "\n",
    "# df/dx = 2x\n",
    "# df/dy = 2y\n",
    "# J = [df/dx, df/dy]\n",
    "\n",
    "# d2f/dx = 2\n",
    "# d2f/dy = 2\n",
    "# d2f/dxdy = 0\n",
    "# d2f/dydx = 0\n",
    "# H = [[d2f/dx, d2f/dxdy], [d2f/dydx, d2f/dy]]\n",
    "\n",
    "def jacobian(f):\n",
    "    return jacrev(f, argnums=(0, 1)) # specify arguments to differentiate wrt\n",
    "\n",
    "def hessian(f):\n",
    "    return jacfwd(jacrev(f, argnums=(0, 1)), argnums =(0, 1))\n",
    "\n",
    "print(f'Jacobian = {jacobian(f)(1., 1.)}') \n",
    "print(f'Full Hessian = {hessian(f)(1., 1.)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9965010-75bb-4d19-9e59-c41ed167849d",
   "metadata": {},
   "source": [
    "_NOTE_: `jacfwd` and `jacrev` compute the same values (up to machine numerics), but differ in their implementation: `jacfwd` uses forward-mode automatic differentiation, which is more efficient for “tall” Jacobian matrices (i.e. number of outputs >> number of variable), while `jacrev` uses reverse-mode, which is more efficient for “wide” Jacobian matrices (i.e. number of outputs << number of variables). For matrices that are near-square, `jacfwd` probably has an edge over `jacrev`. To implement Hessian, we could have used any composition of the `jacfwd` and `jacrev`. But forward-over-reverse is typically the most efficient, because in the inner Jacobian computation we’re often differentiating a function wide Jacobian, while in the outer Jacobian computation we’re differentiating a function with a square Jacobian, which is where forward-mode wins out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeda800-0439-4cc0-a1e0-8a31949cc8b4",
   "metadata": {},
   "source": [
    "# `jit`: just-in-time compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e712f69-dea3-444d-9530-7ef0c0dd9f41",
   "metadata": {},
   "source": [
    "The `jit` transformation performs Just In Time (JIT) compilation of a JAX Python function so it can be executed efficiently in _XLA_. The XLA (short for Accelerated Linear Algebra) compiler takes models from popular frameworks such as PyTorch, TensorFlow, and JAX, and optimizes them for high-performance execution across different hardware platforms including GPUs, CPUs, and ML accelerators. Due to the compilation, which includes fusing of operations, avoidance of allocating temporary arrays, and a host of other tricks, execution times can be orders of magnitude faster in the JIT-compiled case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1988b8-209f-448c-ab01-3794cd99a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple helper visualization function\n",
    "def visualize_fn(fn, l=-10, r=10, n=1000):\n",
    "    x = np.linspace(l, r, num=n)\n",
    "    y = fn(x)\n",
    "    plt.plot(x, y); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f0aed3-35a5-4263-b6fd-eec23089f3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sigmoid activation function - used in NN and classification problems\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + jnp.exp(-x))\n",
    "\n",
    "# Visualize sigmoid\n",
    "visualize_fn(sigmoid)\n",
    "\n",
    "sigmoid_jit = jit(sigmoid)  # let's jit it\n",
    "\n",
    "# Benchmark non-jit vs jit version\n",
    "data = random.normal(key, (1000000,))\n",
    "\n",
    "print('non-jit version:')\n",
    "%timeit sigmoid(data).block_until_ready()\n",
    "print('jit version:')\n",
    "%timeit sigmoid_jit(data).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4277d8a8-d25f-46ac-86d2-ffd95710004f",
   "metadata": {},
   "source": [
    "### Tracing mechanism and its limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba56112-b487-4097-9153-986e9b64b7ec",
   "metadata": {},
   "source": [
    "JIT and other JAX transforms work by _tracing_ a function to determine its effect on inputs of a specific shape and type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1087e9e3-ebe7-4cfd-af39-bdb96d2d1bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit # we can use jit as a decorator, instead of calling it as a function\n",
    "def f(x, y):\n",
    "    print(\"Running f():\")\n",
    "    print(f\"  x = {x}\")\n",
    "    print(f\"  y = {y}\")\n",
    "    result = jnp.dot(x + 1, y + 1)\n",
    "    print(f\"  result = {result}\")\n",
    "    return result\n",
    "\n",
    "x = np.random.randn(3, 4)\n",
    "y = np.random.randn(4)\n",
    "print(f(x, y))\n",
    "\n",
    "x2 = np.random.randn(3, 4)\n",
    "y2 = np.random.randn(4)\n",
    "print('Second call:')\n",
    "print(f(x2, y2))  # Oops! Side effects (like print) are not compiled..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2480f7-7559-4e13-88dd-3fa440e1904e",
   "metadata": {},
   "source": [
    "Notice that the print statements execute, but rather than printing the data we passed to the function, though, it prints tracer objects that stand-in for them.\n",
    "\n",
    "These tracer objects are what `jit` uses to extract the sequence of operations specified by the function. This recorded sequence of computations can then be efficiently applied within XLA to new inputs with the same shape and dtype, without having to re-execute the Python code. Tracers are placeholders that encode the shape and dtype of the arrays, but are independent of their values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d404199-2c60-46af-9742-1552fb9f3014",
   "metadata": {},
   "source": [
    "JAX has its own intermediate representation for sequences of operations, known as a `jaxpr`. A `jaxpr` (short for JAX exPRession) is a simple representation of a functional program, comprising a sequence of primitive operations. We can use `jax.make_jaxpr()` to inspect the `jaxpr` associated to a jit-compiled function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5a20ef-3232-421f-be44-844a77b3f422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same function as above just without the print statements\n",
    "def f(x, y):\n",
    "    return jnp.dot(x + 1, y + 1)\n",
    "\n",
    "print(make_jaxpr(f)(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331cf161-6660-4aee-bf81-0cda594cf32c",
   "metadata": {},
   "source": [
    "A consequence of the tracing mechanism is that not all JAX code can be JIT compiled, as it requires array shapes to be **static** & **known at compile time**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c023ae58-4303-49f6-9e7b-6f7b10a54eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a failure: array shapes must be static\n",
    "\n",
    "def get_negatives(x):\n",
    "    return x[x < 0]\n",
    "\n",
    "x = random.normal(key, (10,), dtype=jnp.float32)\n",
    "print(get_negatives(x))\n",
    "print(jit(get_negatives)(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809f4826-d7c8-4aa9-a327-80379e75c25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd example of a failure:\n",
    "\n",
    "@jit\n",
    "def f(x, neg):  # depends on the value - remember tracer cares about shapes and types!\n",
    "    return -x if neg else x\n",
    "\n",
    "f(1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12233a5-270c-427d-bd63-aef28eecfd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workaround: the \"static\" arguments\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "@partial(jit, static_argnums=(1,))\n",
    "def f(x, neg):\n",
    "    return -x if neg else x\n",
    "\n",
    "print(f(1, True))\n",
    "print(f(2, True))\n",
    "print(f(2, False))\n",
    "print(f(23, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5a7c40-3662-4ab5-9e10-9fc29ae267e1",
   "metadata": {},
   "source": [
    "Note that calling a JIT-compiled function with a different static argument results in re-compilation, so pay attention if you need to call this function with many different values (not a good idea). Static operations are evaluated at compile-time in Python; traced operations are compiled & evaluated at run-time in XLA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6edd2cc-ffc9-4989-8f14-37ac90347d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example: range depends on value again\n",
    "\n",
    "def f(x, n):\n",
    "    y = 0.\n",
    "    for i in range(n):\n",
    "        y += x[i]\n",
    "    return y\n",
    "\n",
    "f_jit = jit(f, static_argnums=(1,))\n",
    "x = (jnp.array([2., 3., 4.]), 1)\n",
    "\n",
    "print(f_jit(*x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80126e32-cbe6-48a1-8353-e851cda93cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd example of a failure:\n",
    "\n",
    "@jit\n",
    "def f(x):\n",
    "    # print(jnp.array(x.shape).prod())\n",
    "    return x.reshape(jnp.array(x.shape).prod())\n",
    "\n",
    "x = jnp.ones((2, 3))\n",
    "f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415d22b8-c3b4-43e5-bd5b-7e4ae5960d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workaround: using numpy instead of jax.numpy\n",
    "\n",
    "@jit\n",
    "def f(x):\n",
    "    return x.reshape((np.prod(x.shape),))\n",
    "\n",
    "f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3e953e-2f70-468e-8809-183387b56115",
   "metadata": {},
   "source": [
    "- JAX is designed to work only with **pure functions**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680d25e3-09d8-42e2-a300-328b034a2578",
   "metadata": {},
   "source": [
    "Informal definition:\n",
    "\n",
    "1. All the input data is passed through the function parameters, all the results are output through the function results.\n",
    "2. A pure function will always return the same result if invoked with the same inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91678a26-54d7-4aa2-a62a-496066c67019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1\n",
    "\n",
    "def impure_print_side_effect(x):\n",
    "    print(\"Executing function\")  # Violating #1\n",
    "    return x\n",
    "\n",
    "# The side-effects appear during the first run \n",
    "print (\"First call: \", jit(impure_print_side_effect)(4.))\n",
    "\n",
    "# Subsequent runs with parameters of same type and shape may not show the side-effect\n",
    "# This is because JAX now invokes a cached compiled version of the function\n",
    "print (\"Second call: \", jit(impure_print_side_effect)(5.))\n",
    "\n",
    "# JAX re-runs the Python function when the type or shape of the argument changes\n",
    "print (\"Third call, different type: \", jit(impure_print_side_effect)(jnp.array([5.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd834cd9-e4f8-4a14-b1af-d6e725d093c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2\n",
    "\n",
    "g = 0.\n",
    "\n",
    "def impure_uses_globals(x):\n",
    "    return x + g  # Violating both #1 and #2\n",
    "\n",
    "# JAX captures the value of the global during the first run\n",
    "print (\"First call: \", jit(impure_uses_globals)(4.))\n",
    "\n",
    "# Let's update the global!\n",
    "g = 10.\n",
    "\n",
    "# Subsequent runs may silently use the cached value of the globals\n",
    "print (\"Second call: \", jit(impure_uses_globals)(5.))\n",
    "\n",
    "# JAX re-runs the Python function when the type or shape of the argument changes\n",
    "# This will end up reading the latest value of the global\n",
    "print (\"Third call, different type: \", jit(impure_uses_globals)(jnp.array([4.])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b29f736-18ef-4db8-aa5a-79a5dc88e3f5",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b1b043-4d9e-4a21-8f88-303d9933fe22",
   "metadata": {},
   "source": [
    "### Exercise 1: gradient descent optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2647d324",
   "metadata": {},
   "source": [
    "Consider a dataset consisting of 2 features and a given number of samples, with the\n",
    "corresponding labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f19bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "key = jax.random.PRNGKey(0)\n",
    "true_w = jnp.array([2.0, -3.0])  # True weights for the synthetic data\n",
    "true_b = 5.0  # True bias for the synthetic data\n",
    "num_samples = 100\n",
    "x = jax.random.normal(key, (num_samples, 2))  # Input features\n",
    "y = jnp.dot(x, true_w) + true_b  # Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d256d64f",
   "metadata": {},
   "source": [
    "Find the weights (2) and bias of a linear model that best fits the dataset (_linear\n",
    "regression_). Complete the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4535637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "def model(weights, bias, x):\n",
    "    pass\n",
    "\n",
    "# Implement the mean squared error loss function\n",
    "def mse_loss(weight, bias, x, y):\n",
    "    pass\n",
    "\n",
    "# Gradient of the loss function\n",
    "grad_mse_loss = ...\n",
    "\n",
    "# Training step\n",
    "@jit\n",
    "def train_step(weights, bias, x, y, learning_rate):\n",
    "    # Update weights and bias using gradient descent\n",
    "    pass\n",
    "\n",
    "# Initialize parameters of the model randomly\n",
    "weights = jax.random.normal(key, (2,))\n",
    "bias = 0.0\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    weights, bias = train_step(weights, bias, x, y, learning_rate)\n",
    "    if epoch % 100 == 0:\n",
    "        current_loss = mse_loss(weights, bias, x, y)\n",
    "        print(f\"Epoch {epoch}, Loss: {current_loss}\")\n",
    "\n",
    "# Print the final parameters\n",
    "print(\"Learned weights:\", weights)\n",
    "print(\"Learned bias:\", bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a3aabb",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebec9456-07c5-404d-affe-015a1e34b312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "def model(weights, bias, x):\n",
    "    return jnp.dot(x, weights) + bias\n",
    "\n",
    "# Mean squared error loss function\n",
    "def mse_loss(weights, bias, x, y):\n",
    "    predictions = model(weights, bias, x)\n",
    "    return jnp.mean((predictions - y) ** 2)\n",
    "\n",
    "# Gradient of the loss function\n",
    "grad_mse_loss = grad(mse_loss, argnums=(0, 1))\n",
    "\n",
    "# Training step\n",
    "@jit\n",
    "def train_step(weights, bias, x, y, learning_rate):\n",
    "    gradients = grad_mse_loss(weights, bias, x, y)\n",
    "    new_weights = weights - learning_rate * gradients[0]\n",
    "    new_bias = bias - learning_rate * gradients[1]\n",
    "    return new_weights, new_bias\n",
    "\n",
    "# Initialize parameters\n",
    "weights = jax.random.normal(key, (2,))\n",
    "bias = 0.0\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    weights, bias = train_step(weights, bias, x, y, learning_rate)\n",
    "    if epoch % 100 == 0:\n",
    "        current_loss = mse_loss(weights, bias, x, y)\n",
    "        print(f\"Epoch {epoch}, Loss: {current_loss}\")\n",
    "\n",
    "# Print the final parameters\n",
    "print(\"Learned weights:\", weights)\n",
    "print(\"Learned bias:\", bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e59599-1d51-4c5c-9a50-bd5a209cc862",
   "metadata": {},
   "source": [
    "**Bonus exercise**: use Optax https://github.com/google-deepmind/optax to solve the same optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2d1eeb-baa0-4c6e-84da-abeb24c5b93a",
   "metadata": {},
   "source": [
    "### Exercise 2: solving a non-linear system of equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26925f9",
   "metadata": {},
   "source": [
    "Consider the following system of non-linear algebraic equations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c951e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system of nonlinear equations\n",
    "@jit\n",
    "def system_of_equations(x):\n",
    "    # Example equations:\n",
    "    # Equation 1: x0^2 + x1 - 37 = 0\n",
    "    # Equation 2: x0 - x1^2 - 5 = 0\n",
    "    return jnp.array([\n",
    "        x[0]**2 + x[1] - 37,\n",
    "        x[0] - x[1]**2 - 5\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1c8635",
   "metadata": {},
   "source": [
    "Solve the system numerically using the [`root`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.root.html)\n",
    "function of `scipy`, with the Jacobian computed via JAX's autodiff. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bfa6e6",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4d4312-7654-45d5-b34f-bcd9aff68d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import root\n",
    "\n",
    "# Function to compute the Jacobian using JAX\n",
    "@jit\n",
    "def jacobian(x):\n",
    "    return jax.jacfwd(system_of_equations)(x)\n",
    "\n",
    "# Initial guess for the solution\n",
    "initial_guess = jnp.array([1.0, 1.0])\n",
    "\n",
    "# Use scipy's root function to find the solution\n",
    "solution = root(system_of_equations, initial_guess, jac=jacobian)\n",
    "\n",
    "# Print the solution\n",
    "print(\"Solution:\", solution.x)\n",
    "print(\"Function value at solution:\", system_of_equations(solution.x))\n",
    "print(\"Jacobian at solution:\", jacobian(solution.x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: introducing state (contextual bandits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the _contextual bandit_ problem, there is a different reward distribution over the\n",
    "actions for each state.  For simplicity, the number of states equals\n",
    "the number of arms, but in general the state space is often much larger than the\n",
    "action space. Here, we have $n$ different reward distributions\n",
    "over actions for each of $n$ states. \n",
    "\n",
    "Instead of storing the rewards for each state-action pair, we use a neural\n",
    "network to learn the relation between state-action and reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random, vmap\n",
    "import numpy as np\n",
    "from flax import nnx\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(av, tau=1.12):\n",
    "    softm = (jnp.exp(av / tau) / jnp.sum(jnp.exp(av / tau)))\n",
    "    return softm\n",
    "\n",
    "def one_hot(N, pos, val=1):\n",
    "    one_hot_vec = jnp.zeros(N)\n",
    "    one_hot_vec = one_hot_vec.at[pos].set(val)\n",
    "    return one_hot_vec\n",
    "\n",
    "def running_mean(x,N=50):\n",
    "    c = x.shape[0] - N\n",
    "    y = np.zeros(c)\n",
    "    conv = np.ones(N)\n",
    "    for i in range(c):\n",
    "        y[i] = (x[i:i+N] @ conv)/N\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Complete the implementation of the `ContextBandit` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextBandit:\n",
    "    def __init__(self, seed=42, arms=10):\n",
    "        key = random.key(seed)\n",
    "        self.arms = arms\n",
    "        self.init_distribution(key, arms)\n",
    "        \n",
    "    def init_distribution(self, key, arms):\n",
    "        self.bandit_matrix = random.uniform(key, shape=(arms,arms))\n",
    "\n",
    "    def reward(self, prob, key):\n",
    "        subkeys = random.split(key, self.arms + 1)\n",
    "        key = subkeys[0]\n",
    "\n",
    "        rewards = vmap(lambda k: random.uniform(k) < prob)(subkeys[1:])\n",
    "        \n",
    "        return rewards.sum(), key \n",
    "\n",
    "    def update_state(self, key):\n",
    "        key, subkey = random.split(key)\n",
    "        state = random.randint(subkey, shape=(), minval=0, maxval=self.arms)\n",
    "        return state, key\n",
    "        \n",
    "    def get_reward(self,arm,state,key):\n",
    "        return self.reward(self.bandit_matrix[arm,state], key)\n",
    "        \n",
    "    def choose_arm(self, arm, state, key):\n",
    "        reward, key = self.get_reward(arm, state, key)\n",
    "        state, key = self.update_state(key)\n",
    "        return reward, state, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arms = 10\n",
    "N, D_in, H, D_out = 1, arms, 100, arms\n",
    "env = ContextBandit(arms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a two-layer neural network that takes as input a one-hot encoded vector of the state and\n",
    "   returns the values (rewards) associated to choosing each arm from that state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nnx.Module):\n",
    "  def __init__(self, D_in, D_out, rngs: nnx.Rngs):\n",
    "    self.linear1 = nnx.Linear(D_in, H, rngs=rngs)\n",
    "    self.linear2 = nnx.Linear(H, D_out, rngs=rngs)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    y = nnx.relu(self.linear1(x))\n",
    "    y = nnx.relu(self.linear2(y))\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define the training loop for the NN model, where at each epoch an action is chosen _probabilistically_\n",
    "   based on the predicted rewards from the model and the model weights are updated based\n",
    "   on the _actual_ reward obtained by taking that action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def _train_epoch(model, optimizer, cur_state, key):\n",
    "    oh_cur_state = one_hot(arms, cur_state)\n",
    "\n",
    "    def loss(model, cur_state, key):\n",
    "        y_pred = model(oh_cur_state)\n",
    "\n",
    "        av_softmax = softmax(y_pred, tau=2.0)\n",
    "\n",
    "        key, subkey = random.split(key)\n",
    "        choice = random.choice(key=subkey, a=arms, p=av_softmax)\n",
    "        cur_reward, cur_state, key = env.choose_arm(choice, cur_state, key)\n",
    "\n",
    "        reward = y_pred.at[choice].set(cur_reward)\n",
    "        MSE = optax.losses.squared_error(y_pred, reward).mean()\n",
    "        return MSE, (cur_reward, cur_state, key)\n",
    "\n",
    "    grads, res = nnx.grad(loss, has_aux=True)(model, cur_state, key)\n",
    "    optimizer.update(grads)\n",
    "\n",
    "    return res\n",
    "\n",
    "def train(env, model, optimizer, epochs=5000):\n",
    "    key = random.key(6)\n",
    "    cur_state, key = env.update_state(key)\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    for _ in range(epochs):\n",
    "\n",
    "        cur_reward, cur_state, key = _train_epoch(model, optimizer, cur_state, key)\n",
    "\n",
    "        rewards.append(cur_reward)\n",
    "\n",
    "    return np.array(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Train the model for 5000 epochs and plot the running mean (use the auxiliary function\n",
    "   defined above) of the rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(D_in, D_out, rngs=nnx.Rngs(0))\n",
    "\n",
    "lr = 0.01 # learning rate\n",
    "optimizer = nnx.ModelAndOptimizer(model, optax.adam(lr))\n",
    "\n",
    "rewards = train(env, model, optimizer)\n",
    "plt.plot(running_mean(rewards,N=500))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The REINFORCE algorithm is generally implemented as an episodic algorithm,\n",
    "meaning that we only apply it to update our model parameters after the agent has\n",
    "completed an entire episode (and collected rewards along the way).\n",
    "By sampling a full episode, we get a pretty good idea of the true value of an action\n",
    "because we can see its downstream effects rather than just its immediate effect (which\n",
    "may be misleading due to randomness in the environment); this full episode sampling\n",
    "is under the umbrella of Monte Carlo approaches.  The advantage of this method is that\n",
    "it’s unbiased. Since we’re not estimating the return, we use only the true return we\n",
    "obtain. However, given the stochasticity of the environment (random events during an\n",
    "episode) and stochasticity of the policy, trajectories can lead to different returns,\n",
    "which can lead to high variance. Consequently, the same starting state can lead to very\n",
    "different returns. Because of this, the return starting at the same state can vary\n",
    "significantly across episodes.\n",
    "\n",
    "Remember that the policy gradient estimation is the direction of the steepest increase in return. In other words, how to update our policy weights so that actions that lead to good returns have a higher probability of being taken. The Monte Carlo variance, which we will further study in this unit, leads to slower training since we need a lot of samples to mitigate it.\n",
    "\n",
    "There are two challenges we want to overcome to\n",
    "increase the robustness of the policy learner:\n",
    "- We want to improve the sample efficiency by updating more frequently.\n",
    "- We want to decrease the variance of the reward we used to update our model.\n",
    "\n",
    "These problems are related, since the reward variance depends on how many samples\n",
    "we collect (more samples yields less variance). The idea behind a combined value-policy\n",
    "algorithm is to use the value learner to reduce the variance in the rewards that are\n",
    "used to train the policy.\n",
    "\n",
    "To address these challenges, we introduce **Actor-Critic** methods, a hybrid architecture combining value-based and Policy-Based methods that helps to stabilize the training by reducing the variance using:\n",
    "\n",
    "- An _Actor_, represented by a function $\\pi_\\theta(s)$ that controls how our agent behaves (Policy-Based method)\n",
    "- A _Critic_, represented by the function $q_w(s,a)$ that measures how good the taken action is (Value-Based method)\n",
    "\n",
    "The Actor-Critic process works like this:\n",
    "1. The Actor (policy) takes the current state $s_t$ and outputs an action $a_t$.\n",
    "2. The Critic takes that $a_t$ and $s_t$ as inputs to compute the Q-value.\n",
    "3. The action $A_t$ performed in the environment outputs a new state $s_{t+1}$ and a\n",
    "   reward $r_{t+1}$.\n",
    "4. The Actor updates its policy parameters using the Q value.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"step4_ac.png\" alt=\"Caption\" width=\"800\" />\n",
    "    <figcaption>Update of the policy paramters (taken from huggingface.co).</figcaption>\n",
    "</figure>\n",
    "\n",
    "5. Thanks to its updated parameters, the Actor produces the next action $A_{t+1}$ to\n",
    "   take given the new state $S_{t+1}$.\n",
    "6. The Critic then updates its value parameters.\n",
    "\n",
    "<!-- <figure>\n",
    "    <img src=\"step5_ac.png\" alt=\"Caption\" width=\"800\" />\n",
    "    <figcaption>Policy gradient objective function (taken from huggingface.co).</figcaption>\n",
    "</figure> -->\n",
    "\n",
    "We can stabilize learning further by using the **Advantage Function** $A(s,a)$ instead\n",
    "of the action-value function in Actor-Critic. We call this method **Advantage Actor\n",
    "Critic** (A2C). The Advantage Function calculates the relative advantage of an action compared to\n",
    "the others\n",
    "possible at a state:\n",
    "\n",
    "$$ A(s,a) = Q(s,a) - V(s) $$\n",
    "\n",
    "with $V(s)$ the state-value function.  In other words, this function calculates the extra reward we get if we take this action at that state compared to the mean reward we get at that state.\n",
    "\n",
    "- If $A(s,a) > 0$: our gradient is pushed in that direction.\n",
    "- If $A(s,a) < 0$ (our action does worse than the average value of that state), our gradient is pushed in the opposite direction.\n",
    "\n",
    "To avoid using both $Q(s,a)$ and $V(s)$, we can estimate $Q(s,a)$ as $r_{t+1} + \\gamma\n",
    "V(s_{t+1})$, with $\\gamma$ the discount factor.\n",
    "\n",
    "We will train a neural network to predict both the policy and the action-value or the state-value.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"ac_scheme.png\" alt=\"Caption\" width=\"800\" />\n",
    "    <figcaption>The general overview of actor-critic methods (taken from \"Deep Reinforcement Learning in Action\")<figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea with Proximal Policy Optimization (PPO) is that we want to improve the training stability of the policy by limiting the change you make to the policy at each training epoch: we want to avoid having too large of a policy update.\n",
    "\n",
    "To do so, we need to measure how much the current policy changed compared to the former one using a ratio calculation between the current and former policy. And we clip this ratio in a range\n",
    "$[1−\\epsilon,1+\\epsilon]$, meaning that we remove the incentive for the current policy\n",
    "to go too far from the old one (hence the proximal policy term).\n",
    "\n",
    "You can read more about this [here](https://huggingface.co/learn/deep-rl-course/unit8/clipped-surrogate-objective)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Stable Baselines JAX (SBX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from sbx import DDPG, DQN, PPO, SAC, TD3, TQC, CrossQ\n",
    "from stable_baselines3.common.logger import configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CartPole-v1, LunarLander-v2, Pendulum-v1, Acrobot-v1, Reacher-v2, Ant\n",
    "environment = \"Ant\"\n",
    "params_cartpole = {}\n",
    "params_pendulum = {\"learning_rate\": 1e-3}\n",
    "params_lunarlander = {\"learning_rate\": 1e-3} # total_timesteps=5e5\n",
    "params_acrobot = {\"learning_rate\": 1e-3, \"gamma\": 0.99, \"gae_lambda\": 0.94} # total_timesteps = 5e5\n",
    "    # \"n_steps\": 512,\n",
    "    # \"batch_size\": 64,\n",
    "    # \"n_epochs\":5,\n",
    "    # \"gamma\":0.999,\n",
    "    # \"gae_lambda\":0.98,\n",
    "    # \"ent_coef\":0.01} # total_timesteps = 5e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env_name, algo_params, total_timesteps=10000, progress_bar=True):\n",
    "    env = gym.make(env_name)\n",
    "    tmp_path = \"tmp/sb3_log/\"\n",
    "    # set up logger\n",
    "    new_logger = configure(tmp_path, [\"csv\"])\n",
    "\n",
    "    model = PPO(\"MlpPolicy\", env, **algo_params)\n",
    "    # Set new logger\n",
    "    model.set_logger(new_logger)\n",
    "    model.learn(total_timesteps=total_timesteps, progress_bar=progress_bar)\n",
    "\n",
    "    # vec_env = model.get_env()\n",
    "    # obs = vec_env.reset()\n",
    "    # total_reward = 0.\n",
    "    # for _ in range(1000):\n",
    "    #     # vec_env.render()\n",
    "    #     action, _states = model.predict(obs, deterministic=True)\n",
    "    #     obs, reward, _, _ = vec_env.step(action)\n",
    "    #     total_reward += reward\n",
    "\n",
    "    # vec_env.close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train(environment, params_cartpole, total_timesteps=5e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"tmp/sb3_log/progress.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_rew_mean = df[\"rollout/ep_rew_mean\"].to_numpy()\n",
    "plt.plot(ep_rew_mean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def play(env_name, model, num_games=1, max_actions=1000, render=False):\n",
    "    if render:\n",
    "        env = gym.make(env_name, render_mode=\"human\")\n",
    "    else:\n",
    "        env = gym.make(env_name)\n",
    "\n",
    "    total_rewards = np.zeros(num_games)\n",
    "    for i in range (num_games):\n",
    "        total_reward = 0\n",
    "        obs = np.array(env.reset()[0])\n",
    "        for _ in range(max_actions):\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            obs = np.array(obs)\n",
    "            if render:\n",
    "                env.render()\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        total_rewards[i] = total_reward\n",
    "\n",
    "    print(\"Average total reward: \", np.mean(total_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(environment, model, num_games=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(environment, model, num_games=1, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

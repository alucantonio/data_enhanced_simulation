{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alucantonio/data_enhanced_simulation/blob/master/9_REINFORCE.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy gradient methods\n",
    "\n",
    "As an alternative to Deep Q-Learning (DQN) where the policy is implicit, we can train a (deep) neural network to\n",
    "output the _probability distribution_ over the actions given a state as an input\n",
    "(**policy network**). Hence, we can directly sample an action from the policy, without\n",
    "the need for an epsilon-greedy strategy. This class of algorithms is called **policy\n",
    "gradient methods** and there are many different types of these.\n",
    "\n",
    "In a _stochastic policy gradient_ method the output of the neural network is an _action vector_ that represents\n",
    "a probability distribution. This allows the agent to explore enough, before the action\n",
    "distribution converges to producing the single best action (degenerate probability\n",
    "distribution), when the environment is stationary (constant distribution of\n",
    "states and rewards).\n",
    "\n",
    "The probability of an action, given the parameters $\\theta$ of the policy network, is\n",
    "denoted $\\pi_s(a|\\theta)$. When training the policy network, we want it to assign more probability to the winning\n",
    "actions, based on a collection of episodes (sequences of states, actions and rewards\n",
    "recorded while interacting with the environment). Our objective then is to maximize the\n",
    "performance of the parameterized policy using **gradient ascent**. To do that, we define an objective function\n",
    "$J(\\theta)$, that is, the **expected cumulative reward** (or return), and we want to find the set of\n",
    "parameters $\\theta$ that maximizes this objective function.\n",
    "\n",
    "Advantages:\n",
    "- Can learn stochastic policies\n",
    "- Do not need to store action-values\n",
    "- Better convergence properties\n",
    "- No perceptual aliasing issue (states that seem (or are) the same but need different actions)\n",
    "- Eï¬€ective in high-dimensional or continuous action spaces\n",
    "\n",
    "Disadvantages:\n",
    "- Typically converge to a local rather than global optimum\n",
    "- Evaluating a policy is typically inefficient and high variance\n",
    "\n",
    "More info [here](https://huggingface.co/learn/deep-rl-course/unit4/hands-on)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"policy_grad_loss.png\" alt=\"Caption\" width=\"800\" />\n",
    "    <figcaption>Policy gradient objective function (taken from huggingface.co).</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we compute the expected cumulative return by summing over all trajectories\n",
    "  $\\tau$ the probability $P(\\tau;\\theta)$ of taking that trajectory given $\\theta$, multiplied by\n",
    "  the _cumulative reward_ $R(\\tau)$ of this trajectory, i.e.\n",
    "  \n",
    "$$ R(\\tau) = \\sum_{k=0}^{T-1} \\gamma^k r_k $$\n",
    "\n",
    "with $\\gamma < 1$ the _discount factor_, and $T$ the number of time steps in the episode.\n",
    "\n",
    "  Thanks to the **Policy Gradient Theorem**, we\n",
    "  can estimate $\\nabla J$ as:\n",
    "\n",
    "  $$\\nabla J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi}[\\nabla \\log \\pi_s(a|\\theta)R(\\tau)]$$\n",
    "\n",
    "  A Monte Carlo estimate of this gradient is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"policy_gradient_multiple.png\" alt=\"Caption\" width=\"800\" />\n",
    "    <figcaption>Policy gradient objective function (taken from huggingface.co).</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The REINFORCE algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The REINFORCE algorithm is a simple policy gradient method. We will apply it to solve\n",
    "the `CartPole` environment (see the docs [here](https://gymnasium.farama.org/environments/classic_control/cart_pole/) and familiarize with the state and actions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from flax import nnx\n",
    "import gymnasium as gym\n",
    "from matplotlib import pyplot as plt\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"reinforce_pseudocode.png\" alt=\"Caption\" width=\"800\" />\n",
    "    <figcaption>Pseudocode of the REINFORCE algorithm (taken from huggingface.co).</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, when we calculate the return $G_t$ (line 6), we sum the discounted rewards starting at\n",
    "timestep $t$. Why? Because our policy should only reinforce actions on the basis of the\n",
    "consequences: so rewards obtained before taking an action are useless (since they were\n",
    "not because of the action), only the ones that come after the action matters. The\n",
    "computation of $G_t$ can be done efficiently using this relation:\n",
    "\n",
    "$$G_t = R_t + \\gamma G_{t+1}$$\n",
    "\n",
    "Here is its implementation in `jax`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def discount_rewards(rewards, gamma=0.99):\n",
    "\n",
    "    def discount_step(carry, reward):\n",
    "        cumulative_reward = reward + gamma * carry\n",
    "        return cumulative_reward, cumulative_reward\n",
    "    \n",
    "    # Use reverse to match the backward computation in the loop\n",
    "    _, discounted_rewards = jax.lax.scan(discount_step, 0.0, rewards, reverse=True)\n",
    "\n",
    "    # Normalize the rewards to be within the [0,1] interval for numerical stability\n",
    "    discounted_rewards /= discounted_rewards.max()\n",
    "    \n",
    "    # Reverse the result to match the original order\n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define the policy network. Use a Leaky-ReLU activation for 1 hidden layer and a softmax\n",
    "function on the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution:\n",
    "\n",
    "class PolicyNetwork(nnx.Module):\n",
    "  def __init__(self, l1, l2, l3, rngs: nnx.Rngs):\n",
    "    self.linear1 = nnx.Linear(l1, l2, rngs=rngs)\n",
    "    self.linear2 = nnx.Linear(l2, l3, rngs=rngs)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    y = nnx.leaky_relu(self.linear1(x))\n",
    "    y = self.linear2(y)\n",
    "    y = nnx.softmax(y)\n",
    "    return y\n",
    "\n",
    "l1 = 4\n",
    "l2 = 150\n",
    "l3 = 2\n",
    "\n",
    "learning_rate = 0.002\n",
    "model = PolicyNetwork(l1, l2, l3, nnx.Rngs(0))\n",
    "optimizer = nnx.Optimizer(model, optax.adam(learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define the loss function `loss_fn` and the `train_step` function that performs one update of the\n",
    "weights of the policy network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution:\n",
    "\n",
    "@nnx.jit\n",
    "def loss_fn(model, states, actions, discounted_rewards):\n",
    "    log_probs = jnp.log(jnp.take_along_axis(model(states), actions[:, None], axis=1).squeeze())\n",
    "    return -jnp.sum(log_probs * discounted_rewards)\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model, optimizer, states, actions, discounted_rewards):\n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(model, states, actions, discounted_rewards)\n",
    "    optimizer.update(grads)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Complete the main training loop and train the policy network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DUR = 200\n",
    "MAX_EPISODES = 500\n",
    "gamma = 0.99\n",
    "score = np.zeros(MAX_EPISODES)\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    curr_state = env.reset()[0]\n",
    "    states, actions = [], []\n",
    "\n",
    "    states = np.zeros((MAX_DUR, 4))\n",
    "    actions = np.zeros(MAX_DUR, dtype=np.int32)\n",
    "    rewards = np.zeros(MAX_DUR)\n",
    "\n",
    "    num_t = MAX_DUR \n",
    "    \n",
    "    for t in range(MAX_DUR):\n",
    "        # Estimate action probabilities for the current state using the policy network\n",
    "        # ...\n",
    "\n",
    "        # Perform action and get new state and terminated/truncated flags from the environment\n",
    "        # ...\n",
    "\n",
    "        # Record episode data (states and actions)\n",
    "        # states[t] ...\n",
    "        # actions[t] ...\n",
    "\n",
    "        # Update the current state with the next state got from the environment\n",
    "        curr_state = next_state\n",
    "\n",
    "        if terminated or truncated:\n",
    "            num_t = t + 1\n",
    "            break\n",
    "\n",
    "    # Compute episode score and array of rewards\n",
    "    # score[episode] = ... \n",
    "    # rewards = ...\n",
    "\n",
    "    # Compute discounted future rewards\n",
    "    # disc_future_rewards = ...\n",
    "\n",
    "    # Update policy parameters\n",
    "    loss = train_step(model, optimizer, states, actions, disc_future_rewards) \n",
    "        \n",
    "    if episode % 100 == 0:\n",
    "        # Print episode number and total reward\n",
    "        # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution:\n",
    "\n",
    "MAX_DUR = 200\n",
    "MAX_EPISODES = 500\n",
    "gamma = 0.99\n",
    "score = np.zeros(MAX_EPISODES)\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    curr_state = env.reset()[0]\n",
    "\n",
    "    states = np.zeros((MAX_DUR, 4))\n",
    "    actions = np.zeros(MAX_DUR, dtype=np.int32)\n",
    "    rewards = np.zeros(MAX_DUR)\n",
    "\n",
    "    num_t = MAX_DUR \n",
    "    \n",
    "    for t in range(MAX_DUR):\n",
    "        act_prob = model(curr_state)\n",
    "        action = np.random.choice(np.array([0,1]), p=act_prob.__array__())\n",
    "\n",
    "        next_state, _, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # Record episode data\n",
    "        states[t] = curr_state\n",
    "        actions[t] = action\n",
    "\n",
    "        curr_state = next_state\n",
    "\n",
    "        if terminated or truncated:\n",
    "            num_t = t + 1\n",
    "            break\n",
    "\n",
    "    ep_len = num_t\n",
    "    score[episode] = ep_len\n",
    "    rewards[:ep_len] = 1.\n",
    "\n",
    "    # pick only the meaningful rewards (needed to properly compute loss)\n",
    "    mask = rewards\n",
    "\n",
    "    disc_future_rewards = discount_rewards(rewards)*mask\n",
    "\n",
    "    loss = train_step(model, optimizer, states, actions, disc_future_rewards) \n",
    "        \n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode}, Total Reward: {ep_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Plot the running mean of the scores (episode duration) obtained during the training episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, N=50):\n",
    "    kernel = np.ones(N)\n",
    "    conv_len = x.shape[0]-N\n",
    "    y = np.zeros(conv_len)\n",
    "    for i in range(conv_len):\n",
    "        y[i] = kernel @ x[i:i+N]\n",
    "        y[i] /= N\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_score = running_mean(score, 50)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.ylabel(\"Episode Duration\",fontsize=22)\n",
    "plt.xlabel(\"Training Epochs\",fontsize=22)\n",
    "plt.plot(avg_score, color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "games = 100\n",
    "done = False\n",
    "state1 = env.reset()[0]\n",
    "for i in range(games):\n",
    "    t=0\n",
    "    while True:\n",
    "        if type(state1) is tuple:\n",
    "            state1 = state1[0]\n",
    "        pred = model(state1)\n",
    "        action = np.random.choice(np.array([0,1]), p=pred.__array__())\n",
    "        state2, reward, terminated, truncated, info = env.step(action)\n",
    "        state1 = state2\n",
    "        if(type(state1) == 'tuple'):\n",
    "            state1 = state2[0]\n",
    "        \n",
    "        t += 1\n",
    "        if t > MAX_DUR or truncated or terminated:\n",
    "            break;\n",
    "    state1 = env.reset()\n",
    "    done = False\n",
    "    score.append(t)\n",
    "\n",
    "score = np.array(score)\n",
    "plt.scatter(np.arange(score.shape[0]),score)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus exercise**: try to solve the [Taxi](https://gymnasium.farama.org/environments/toy_text/taxi/) environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

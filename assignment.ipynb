{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-Enhanced Simulation for Solids - final assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goals of this assignment are to study a heat conduction problem, solve it using a\n",
    "neural network, and develop a model-free controller based on reinforcement learning.\n",
    "As a solution to the assignment, please submit a **Jupyter notebook**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the one-dimensional diffusion equation with source term:\n",
    "\n",
    "$$ \\frac{\\partial u}{\\partial t}(x,t) = \\frac{\\partial^2 u}{\\partial x^2}(x,t) +\n",
    "\\lambda(x) u(x,t) \\qquad 0<x<1\\,, 0<t\\leq 1,$$\n",
    "\n",
    "where $\\lambda(x) = 50 \\cos(8\\arccos(x))$. The boundary conditions are $u(0,t) = 0$ and\n",
    "$u(1,t) = a(t)$, with $a(t)$ a **control function**, while the initial condition is\n",
    "$u(x,0) = 1$. Notice that the source term has a _destabilizing_ effect on the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Solve the diffusion problem with finite differences (forward first-order in time,\n",
    "   centered second-order in space) with constant control $a(t) = 0$ and make a\n",
    "   3D plot of the solution $u$ for $(x,t) \\in [0,1] \\times [0,1]$. Discretize the space\n",
    "   with $\\Delta x = 0.005$ and time with $\\Delta t = 10^{-5}$.\n",
    "2. Use a neural network to solve the same problem (given only the boundary and the\n",
    "   initial conditions) and evaluate the mean squared error over the space-time interval between the network\n",
    "   prediction and the numerical solution obtained previously.\n",
    "3. Create an `gymnasium` environment that simulates the diffusion problem (using the\n",
    "   numerical solver developed in step 1) and is\n",
    "   suitable for applying reinforcement learning algorithms, following the template\n",
    "   below. The _state_ $s$ corresponds to the field $u(x,t)$, while the _action_ is the\n",
    "   choice of the value for the boundary control $a(t)$ at each time step. \n",
    "   The _reward_ should be such that the control tries to **stabilize** the solution\n",
    "   $u(x,t)$, i.e. it should compensate for the source term and drive the solution\n",
    "   towards zero. Precisely, the reward should be assigned as:\n",
    "   - $\\|s'\\| - \\|s\\|$, for each timestep, where $s'$ is the state after performing an action and $s$ is the\n",
    "     state before it, and $\\|\\cdot\\|$ denotes the $L_2$ norm;\n",
    "   - $300 - \\|u(x,1)\\| - \\sum_{t=0}^1 |a(t)|/1000$, at the end of the episode if $\\|u(x,1)\\| < 20$,\n",
    "     or $0$ otherwise, with $|\\cdot|$ the $L_1$ norm.\n",
    "   \n",
    "   Actions are sampled every $0.01$ time units. Since the timestep for the numerical\n",
    "   integration of the PDE ($10^{-5}$) is smaller the the control sampling timestep, the control is kept constant until\n",
    "   reaching the next time multiple of $0.01$. It is advisable to test the enviroment\n",
    "   before using it for reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import Env, spaces\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "\n",
    "dx = 0.005\n",
    "dt = 1e-5\n",
    "control_dt = 0.01  # Control sampling timestep\n",
    "x = jnp.arange(0, 1 + dx, dx)\n",
    "nx = len(x)\n",
    "num_timesteps = int(1 / dt)  # Total simulation time of 1 second\n",
    "num_control_steps = int(control_dt / dt)  # Number of PDE steps per control action\n",
    "\n",
    "@jit\n",
    "def source_term(x, u):\n",
    "    return 50 * jnp.cos(8 * jnp.arccos(x)) * u\n",
    "\n",
    "class DiffusionEnv(Env):\n",
    "    def __init__(self):\n",
    "        super(DiffusionEnv, self).__init__()\n",
    "        \n",
    "        # Define action and observation spaces\n",
    "        self.action_space = spaces.Box(low=-20, high=20, shape=(1,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(nx,), dtype=np.float32)\n",
    "        \n",
    "        # Initialize state\n",
    "        # self.state = ...\n",
    "\n",
    "        self.current_step = 0  # Track the number of time steps\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        self.state = jnp.ones(nx, dtype=jnp.float32)\n",
    "        self.state = self.state.at[0].set(0)  # Enforce boundary condition at x=0\n",
    "        self.current_step = 0\n",
    "        return np.array(self.state), {}  # Convert to numpy for compatibility with stable-baselines\n",
    "\n",
    "    def step(self, action):\n",
    "        # Apply boundary control at x=1\n",
    "        boundary_control = action[0]\n",
    "\n",
    "        # Update state (i.e. integrate PDE for num_control steps)\n",
    "        # self.state = ...\n",
    "\n",
    "        new_state = np.array(self.state)\n",
    "\n",
    "        # Calculate reward: Negative squared norm of state difference\n",
    "        # reward = ...\n",
    "\n",
    "        # If this is the last timestep, adjust the reward based on the final L2 norm\n",
    "        if self.current_step >= num_timesteps // num_control_steps:\n",
    "            # final_norm = ...\n",
    "            if final_norm <= 20:\n",
    "                # reward = ...\n",
    "            else:\n",
    "                reward = 0\n",
    "\n",
    "        # The episode only ends after the full duration (1 second)\n",
    "        done = self.current_step >= num_timesteps // num_control_steps\n",
    "        truncated = False  # No truncation\n",
    "\n",
    "        # Increment the timestep counter\n",
    "        self.current_step += 1\n",
    "\n",
    "        return new_state, reward, done, truncated, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use a reinforcement learning algorithm to stabilize the solution by acting on the boundary control. The algorithm should interact with the environment built at step 3. Plot the reward as a function of the epochs or total number of timesteps during training of the algorithm. Also make a 3D plot of the stabilized solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following snippet could be useful if using stable-baselines\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor\n",
    "\n",
    "# Wrap the environment with VecMonitor for reward logging\n",
    "env = DiffusionEnv()\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecMonitor(env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
